%
% (c) 2025 Autor, ETH ZÃ¼rich
%
% !TEX root = main.tex
% !TEX encoding = UTF-8
%

\section{The Derivative and the Riemann Integral}

\subsection{The Fundamental Theorem of Calculus}
Throughout this section we fix a compact interval $I \subseteq\mathbb{R}$ that is non-emtpy and contains more than one point. For brevity, we write \textit{integrable} for \textit{Riemann integrable}.

\subsubsection{The Fundamental Theorem}
\begin{definition}{Primitive Function}{primitive_func}
	Let $I \subseteq \mathbb{R}$ be an interval and $f:I \to \mathbb{R}$ a function. Any differentiable function $F : I \to \mathbb{R}$ such that $F' = f$ is called a \textbf{primitive} (or \textbf{antiderivative}) of $f$.
\end{definition}

\begin{remark}
	A primitive may not always exist.
\end{remark}

The next result is known as the \textbf{Fundamental Theorem of (Integral and Differential) Calculus}, going back to Leibniz, Newton and Barrow.

\begin{theorem}{Fundamental Theorem of Calculus}{fund_thm_calc}
	Let $f:[a,b] \to \mathbb{R}$ be continuous. Then:
	\begin{enumerate}
		\item[(i)] For every $C \in \mathbb{R}$, the function $F:[a,b] \to \mathbb{R}$ defined by
		\begin{equation}
			\label{eq:primitive}
			F(x) = \int_{a}^{x} f(t) \, dt + C
		\end{equation}
		is a primitive of $f$.
		\item[(ii)] Every primitive $F:[a,b] \to \mathbb{R}$ of $f$ has the form \eqref{eq:primitive} for some constant $C$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	By Theorem \ref{theo*cont_func_int}, $f$ is integrable.
	
	Let $F$ be defined as in Equation \eqref{eq:primitive}. To prove (i), we fix $x_0 \in [a,b]$ and we want to show that $F'(x_0) = f(x_0)$. To this aim, fix $\varepsilon > 0$. By continuity, there exists $\delta > 0$ such that
	\begin{equation}
		\label{eq:fund_cont}
		z \in [a,b], \quad |z - x_0| < \delta \quad \Rightarrow \quad |f(z) - f(x_0)| < \varepsilon.
	\end{equation}
	Now, given $x \in (x_0, x_0 + \delta) \cap [a,b]$, it follows from Remark \ref{rmk:int_seperate} that
	\begin{align*}
		\left|\frac{F(x) - F(x_0)}{x - x_0} - f(x_0)\right| &= \left|\frac{1}{x - x_0} \bigg(\int_{a}^{x} f(t) \, dt - \int_{a}^{x_0} f(t) \, dt\bigg) - f(x_0)\right|\\
		&= \left|\frac{1}{x- x_0} \int_{x_0}^{x} f(t) \, dt - f(x_0)\right|.
	\end{align*}
	Also,
	\[
		f(x_0) = f(x_0) \frac{1}{x - x_0} \int_{x_0}^{x} \, dt = \frac{1}{x - x_0} \int_{x_0}^{x} f(x_0) \, dt.
 	\]
 	Combining these two equations and using Theorem \ref{theo*triangle_ineq_int}, we get
 	\begin{align*}
 		\left|\frac{F(x) - F(x_0)}{x - x_0} - f(x_0)\right| &= \left|\frac{1}{x - x_0} \int_{x_0}^{x} f(t) \, dt - \frac{1}{x - x_0} \int_{x_0}^{x} f(x_0)\, dt\right|\\
 		&= \left|\frac{1}{x - x_0} \int_{x_0}^{x} (f(t) - f(x_0)) \, dt\right| \\
 		&\leq \frac{1}{x - x_0} \int_{x_0}^{x} |f(t) - f(x_0)| \, dt.
 	\end{align*}
 	Note now that, in the last integral, $t \in [x_0, x] \subseteq [x_0, x_0 + \delta) \cap [a,b]$. Hence, it follows form Equation \eqref{eq:fund_cont} that $|f(t) - f(x_0)| < \varepsilon$, therefore
 	\[
 		\left|\frac{F(x) - F(x_0)}{x - x_0} - f(x_0)\right| < \frac{1}{x - x_0} \int_{x_0}^{x} \varepsilon \, dt = \varepsilon. 
 	\]
 	Similarly, if $x \in (x_0 - \delta, x_0) \cap [a,b]$, then
 	\[
 		\left|\frac{F(x) - F(x_0)}{x - x_0} - f(x_0)\right| = \left|\frac{1}{x_0 - x} \int_{x}^{x_0} (f(t) - f(x_0)) \, dt\right| \leq \frac{1}{x_0 - x} \int_{x}^{x_0} |f(t) - f(x_0)| \, dt < \varepsilon.
 	\]
 	In summary, we proved that
 	\[
 		x \in [a,b], \quad |x - x_0| < \delta \quad \Rightarrow \quad \left|\frac{F(x) - F(x_0)}{x - x_0} - f(x_0)\right| < \varepsilon,
 	\]
 	therefore
 	\[
 		F'(x_0) = \lim_{x \to x_0} \frac{F(x) - F(x_0)}{x - x_0} = f(x_0),
 	\]
 	as desired.
 	
 	We now prove (ii). Let $F$ be a primitive of $f$. Then, since $\left(\int_{a}^{x} f(t) \, dt\right)' = f(x)$ (by (i)), we have that
 	\[
 		\left(F(x) - \int_{a}^{x} f(t)\, dt\right)' = F'(x) - f(x) = f(x) - f(x) = 0 \qquad  \forall x \in (a,b).
 	\]
 	By Corollary \ref{cor*const_deriv}, this implies that $F(x) - \int_{a}^{x} f(t)\, dt$ is constant on $[a,b]$, concluding the proof of (ii). \qedhere
\end{proof}

\begin{corollary}{Integral vs. Derivative}{int_vs_deriv}
	If $F:[a,b] \to \mathbb{R}$ is continuously differentiable, then for all $x \in [a,b]$,
	\[
		F(x) = F(a) + \int_{a}^{x} F'(t)\, dt.
	\]
\end{corollary}

\begin{proof}
	Since $F$ is a primitive of $F'$, Theorem \ref{theo*fund_thm_calc} yields $F(x) = \int_{a}^{x} F'(t)\, dt + C$. Evaluating at $x = a$ gives $C = F(a)$. \qedhere
\end{proof}

\begin{corollary}{Riemann Integral and Primitives}{int_primitive}
	If $f:[a,b] \to \mathbb{R}$ is continuous and $F$ is a primitive of $f$, then
	\[
		\int_{a}^{b} f(t)\, dt = F(b) - F(a).
	\]
\end{corollary}

\begin{proof}
	Apply Corollary \ref{cor*int_vs_deriv} with $F' = f$ and $x = b$. \qedhere
\end{proof}

\subsubsection{Integration by Parts and Substitution}
Given a Function $h:[a,b] \to \mathbb{R}$, we use the notation $\big[h(x)\big]_a^b := h(b) - h(a)$.

\begin{theorem}{Integration by Parts}{int_parts}
	If $f,g:[a,b] \to \mathbb{R}$ are continuously differentiable, then
	\[
		\int_{a}^{b} f(x) g'(x) \, dx = \big[f(x)g(x)\big]_a^b - \int_{a}^{b} f'(x)g(x)\, dx.
	\]
\end{theorem}

\begin{proof}
	By Proposition \ref{prop*derivative_sum_prod}, $(fg)' = f'g + fg'$. Rearranging and integrating, thanks to Corollary $\ref{cor*int_primitive}$, we get
	\[
		\int_{a}^{b} fg'\, dx = \int_{a}^{b} (fg)'\, dx - \int_{a}^{b}f g'\, dx = \big[fg\big]_a^b - \int_{a}^{b} fg'\, dx. \qedhere
	\]
\end{proof}

As a convention, for any $h:[a,b] \to \mathbb{R}$,
\begin{equation}
	\label{eq:convention_int}
	\int_{b}^{a} h(x)\, dx = -\int_{a}^{b} h(x)\, dx.
\end{equation}

\begin{theorem}{Integration by Substitution, 1st Form}{int_sub1}
	Let $I,J \subseteq \mathbb{R}$ be intervals, $f:I \to J$ be continuously differentiable, and $g:J \to \mathbb{R}$ be continuous. For any $[a,b] \subseteq I$,
	\[
		\int_{a}^{b} g(f(x))f'(x) \, dx = \int_{f(a)}^{f(b)} g(y) \, dy.
	\]
\end{theorem}

\begin{proof}
	Fix $y_0 \in J$ and set $G(y) = \int_{y_0}^{y} g(t)\, dt$. Since $G' = g$, by the chain rule (Theorem \ref{theo*chain_rule}) we get $(G \circ f)' = G'(f)f' = g(f)f'$. Integrating this identity and using Corollary \ref{cor*int_primitive} yields
	\begin{align*}
		\int_{a}^{b} g(f(x))f'(x)\, dx &= \int_{a}^{b} (G\circ f)'(x)\, dx = G(f(b)) - G(f(a))\\
		&= \int_{y_0}^{f(b)} g(t)\, dt - \int_{y_0}^{f(a)} g(t)\, dt = \int_{f(a)}^{f(b)} g(t)\, dt.\qedhere
	\end{align*}
\end{proof}

Before stating the nect result, we note the following: If $h:[a,b]\to \mathbb{R}$ is continuously differentiable with $h' \neq 0$, then $h'$ has constant sign on $[a,b]$, so $h$ is strictly monotone and invertible ; $h^{-1}$ is continuous by Theorem \ref{theo*inv_func_theo} and differentiable on $(h(a), h(b))$ by Theorem \ref{theo*derivative_inv}. Furthermore, since$(h^{-1})' = \frac{1}{h'\circ h^{-1}}$, also $(h^{-1})'$ is continuous.

\begin{theorem}{Integration by Substitution, 2nd Form}{int_sub2}
	Let $I,J \subseteq \mathbb{R}$ be intervals, $f:I \to J$ be $C^1$, and $g:J \to \mathbb{R}$ be continuous. Let $[a,b] \subseteq I$ and assume $f'(x) \neq 0$ on $[a,b]$. If $f^{-1}:[f(a), f(b)] \to \mathbb{R}$ denotes the inverse of $f|_{[a,b]}$, then
	\[
		\int_{a}^{b} g(f(x))\, dx = \int_{f(a)}^{f(b)} g(y)(f^{-1})'(y) \, dy.
	\]
\end{theorem}

\begin{proof}
	In order to apply Theorem \ref{theo*int_sub1}, we first observe that
	\[
		\int_{a}^{b} g(f(x))\, dx = \int_{a}^{b} \frac{g(f(x))}{f'(x)}f'(x)\, dx = \int_{a}^{b}\frac{g(f(x))}{f' \circ f^{-1}(f(x))}f'(x)\, dx.	
	\]
	So we can apply Theorem \ref{theo*int_sub1} with $\frac{g}{f' \circ f^{-1}}$ in place of $g$ to get
	\[
		\int_{a}^{b} g(f(x)) \, dx = \int_{f(a)}^{f(b)} \frac{g(y)}{f'(f^{-1}(y))}\, dy.
	\]
	Since $\frac{1}{f'\circ f^{-1}} = (f^{-1})'$, the result follows. \qedhere
\end{proof}

\subsubsection{Improper Integrals}
A function $f:I \to \mathbb{R}$ is \textbf{locally integrable} if $f|_{[a,b]}$ is integrable for every compact interval $[a,b]\subseteq I$.

\begin{definition}{Improper Integrals}{improper_int}
	Let $I \subseteq \mathbb{R}$ be a non-empty interval and $f:I \to \mathbb{R}$ be locally integrable. Set $c = \inf I \in \mathbb{R}\cup \{-\infty\}$ and $d = \sup I \in \mathbb{R}\cup \{\infty\}$, and fix $x_0 \in I$. We define the \textbf{improper integral} of $f$ on $I$ by
	\[
		\int_{c}^{d} f(x)\, dx := \lim_{a \to c^+}\int_{a}^{x_0}f(x) \, dx + \lim_{b \to d^-} \int_{x_0}^{b} f(x)\, dx,
	\]
	whenever both limits exist and the sum is well-defined (we do not allow the indeterminate form $\infty - \infty$). Here the first limit is taken over $a \in I$ with $c < a < x_0$ (interpreting $a \to -\infty$ if $c = -\infty$) and the second over $b \in I$ with $x_0 < b < d$ (interpreting $b \to +\infty$ if $d = \infty$). If the value is finite we say the integral \textit{converges}; if it is $\pm \infty$ we say it \textit{diverges to} $\pm \infty$; otherwise, it \textit{does not converge}. When defined, the value is independent of the choice of $x_0$. 
\end{definition}

\begin{lemma}{Improper Integrals of Non-negative Functions}{improper_int_non_neg}
	Let $I \subseteq \mathbb{R}$ be a non-empty interval with $c = \inf I$ and $d = \sup I$, and let $f:I \to [0, \infty)$ be locally integrable. Fix any $x_0 \in I$. Then the one-sided limits
	\[
		L_- := \lim_{a \to c^+} \int_{a}^{x_0} f(x)\, dx, \qquad L_+ := \lim_{b \to d^-} \int_{x_0}^{b} f(x) \, dx
	\]
	exist in $[0,\infty]$, and
	\[
		\int_{c}^{d} f(x)\, dx = L_- + L_+ = \sup_{c < \alpha < \beta < d}\, \int_{\alpha}^{\beta} f(x)\, dx \in [0,\infty].
	\]
	In particular, the improper integral over $I$ always exists in the extended sense and equals $+\infty$ whenever the supremum is $+\infty$.
\end{lemma}

\begin{proof}
	Fix $x_0 \in I$. Since $f \geq 0$, the maps $a \mapsto \int_{a}^{x_0}f\, dx$ (for $a < x_0$) and $b \mapsto \int_{x_0}^{b}f\, dx$ (for $b > x_0$) are monotone, hence the limits $L_-, L_+$ exist in $[0,\infty]$. Also, for any $c < a < x_0 < b < d$,
	\[
		\int_{a}^{b} f\, dx = \int_{a}^{x_0} f\, dx + \int_{x_0}^{b}f\, dx.
	\]
	(this is different from the skript. Idk if theres a typo in the skript.)
	Taking suprema gives
	\[
		\sup_{c < \alpha < \beta < d}\, \int_{\alpha}^{\beta}f\, dx = \sup_{a < x_0} \int_{a}^{x_0} f\, dx + \sup_{b > x_0}\int_{x_0}^{b}f\, dx = L_- + L_+,
	\]
	which equals the definition of $\int_{c}^{d}f\, dx$ above.\qedhere
\end{proof}

\begin{theorem}{Integral Test for Series}{int_test_series}
	Let $f:[0,\infty) \to [0,\infty)$ be monotone decreasing. Then for every $n \in \mathbb{N}$,
	\[
		\sum_{n=1}^{N+1} f(n)\leq \int_{0}^{N+1}f(x)\, dx \leq \sum_{n=0}^{N}f(n).
	\]
	In particular, 
	\[
		\sum_{n=0}^{\infty} f(n) \text{ converges}\qquad \Leftrightarrow \qquad \int_{0}^{\infty}f(x)\, dx \text{ converges.}
	\]
\end{theorem}

\begin{proof}
	By monotonicity, $f$ is locally integrable. Define step functions on $[0,\infty)$ by 
	\[
		u(x) = f(\lfloor x \rfloor),\qquad \ell(x) = f(\lceil x \rceil),
	\]
	where $\lfloor x \rfloor$ is the rounding function (,i.e., the largest integer $\leq x$), while $\lceil x \rceil$ denotes the smallest integer $\geq x$. Then $\ell \leq f \leq u$, and for $N \geq 1$,
	\[
		\sum_{n=1}^{N+1}f(n) = \int_{0}^{N+1}\ell(x)\ dx \leq \int_{0}^{N+1}f(x)\, dx \leq \int_{0}^{N+1}u(x) = \sum_{n=0}^{N}f(n).
	\]
	The result follows by taking the limit as $N \to \infty$.\qedhere
\end{proof}

\subsection{Integration and Differentiation of Power Series}
We recall the limit
\begin{equation}
	\label{eq:lim_nroot_n}
	\lim_{n\to \infty} \sqrt[n]{\frac{1}{n}} = 1.
\end{equation}
Also we shall use the following fact.

\begin{remark}
	\label{rmk:limsup_seq}
	Let $(a_n)_{n=0}^{\infty}$ be a sequence of non-negative numbers with
	\[
		L = \limsup_{n \to \infty} a_n < \infty,
	\]
	and let $(b_n)_{n=0}^{\infty}$ and $(\gamma_n)_{n=0}^{\infty}$ satisfy $b_n \to 1$ and $\gamma_n \to 1$. Then
	\[
		\limsup_{n \to \infty} a_n^{\gamma_n}b_n = L.
	\]
	In other words, multiplying by a factor that tends to 1, or raising to an exponent that tends to 1, does not change the value of the lim sup.
\end{remark}

\begin{theorem}{Integration of Power Series}{int_pwr_series}
	Let $f(x) = \sum_{n=0}^{\infty}a_nx^n$ have radius of convergence $R > 0$. Then 
	\[
		F(x) = \sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}
	\]
	has the same radius of convergence $R$ and is a primitive of $f$ on $(-R,R)$.
\end{theorem}

\begin{proof}
	Set $\rho = \limsup_{n \to \infty} \sqrt[n]{|a_n|}$, so that $R = \rho^{-1}$. Define $c_0 = 0$ and $c_n = \frac{a_{n-1}}{n}$ for $n\geq 1$, so $F(x) = \sum_{n=0}^{\infty}c_n x^n$. Noticing that
	\[
		\sqrt[n]{|c_n|} = \sqrt[n]{\frac{1}{n}}\left(\sqrt[n-1]{|a_{n-1}|}\right)^{\frac{n-1}{n}},
	\]
	it follows from Equation \eqref{eq:lim_nroot_n} and Remark \ref{rmk:limsup_seq} (applied with $b_n = \sqrt[n]{\frac{1}{n}}$ and $\gamma_n = \frac{n-1}{n}$) that
	\[
		\limsup_{n \to \infty} \sqrt[n]{|c_n|} = \limsup_{n \to \infty} \sqrt[n-1]{|a_{n-1}|} = \rho,
	\]
	hence also $F$ has radius of convergence $R$.
	
	We now want to prove that $F' = f$. Fix $[-r, r] \subseteq (-R, R)$ and define the polynomials $f_n(x) = \sum_{k=0}^{n} a_kx^k$. Then
	\[
		\int_{0}^{x} f_n(t)\, dt = \sum_{k=0}^{n}\frac{a_k}{k+1}x^{k+1}.
	\]
	By Theorem \ref{theo*cont_pwr_series}, the sequence of functions $(f_n)_{n=0}^{\infty}$ converge uniformly to $f$ on $[-r, r]$, so Theorem \ref{theo*unif_conv_int_commute} yields
	\[
		\int_{0}^{x} f(t)\, dt = \lim_{n \to \infty} \int_{0}^{x} f_n(t)\, dt \qquad \forall x\in [-r, r].
	\]
	On the other hand, again by Theorem \ref{theo*cont_pwr_series},
	\[
		\lim_{n \to \infty} \sum_{k=0}^{n} \frac{a_k}{k+1}x^{k+1} = F(x) \qquad \forall x \in [-r, r].
	\]
	This proves that $F(x) = \int_{0}^{x}f(t)\, dt$ on $[-r,r]$, so Theorem \ref{theo*fund_thm_calc} implies that $F'(x) = f(x)$ on $[-r, r]$. Since $[-r, r]\subseteq (-R, R)$ is arbitrary, we proved that $F'=f$ on $(-R, R)$.\qedhere
\end{proof}

\begin{corollary}{Differentiation of Power Series}{diff_pwr_series}
	Let $f(x) = \sum_{n=0}^{\infty}a_nx^n$ have radius of convergence $R > 0$. Then $f$ is differentiable on $(-R,R)$ with
	\[
		f'(x) = \sum_{n=1}^{\infty}na_nx^{n-1}\qquad \forall x \in (-R,R),
	\]
	and the series on the right has radius of convergence $R$.
\end{corollary}

\begin{proof}
	Let $c_n = (n+1) a_{n+1}$ and $g(x) = \sum_{n=1}^{\infty} na_n x^{n-1} = \sum_{n=0}^{\infty}c_n$. Let $\bar{R}$ be the radius of convergence of $g$. Then Theorem \ref{theo*int_pwr_series} implies that the power series
	\[
		G(x) = \sum_{n=0}^{\infty} \frac{c_n}{n+1}x^{n+1}
	\]
	has radius $\bar{R}$ and is a primitive of $g$. But
	\[
		G(x) = \sum_{n=0}^{\infty}\frac{(n+1)a_{n+1}}{n+1}x^{n+1} = \sum_{n=0}^{\infty} a_{n+1}x^{n+1} = \sum_{n=1}^{\infty}a_nx^n = f(x) - a_0.
	\]
	This implies that $G$ and $f$ have the same radius of convergence (so $\bar{R} = R$) and that $g = G' = (f - a_0)' = f'$.\qedhere
\end{proof}

\subsection{Integration Methods}
Let $I \subseteq \mathbb{R}$ be an interval, and $f:I \to \mathbb{R}$ a function. The notation
\[
	\int f(x)\, dx = F(x) + C
\]
means that $F$ is a primitive function (antiderivative) of $f$. In the expression $F(x) + C$, $C$ is read as an indefinite constant, usually called \textbf{integration constant}. Since the domain $I$ of $f$ is an interval, two primitive functions of $f$ differ by a constant, which makes the notation meaningful. One calls $F(x) + C$ the \textbf{indefinite integral} of $f$. In this section we summarize general methods to determine indefinite integrals.

Throughout this section, $I \subseteq \mathbb{R}$ denotes a non-empty interval that is not a single point. Also, all functions in this section are real-valued functions with domain $I$ that are integrable on any compact interval $[a,b] \subseteq I$.

\subsubsection{Integration by Parts and by Substitution in Leibniz Notation}
We recall the derivative of a function $h$ is denoted by $h'$ or by $\frac{dh}{dx}$.
\subsubsection*{Integration by Parts}
Let $f$ and $g$ be functions with primitives $F$ and $G$, respectively. Recall the Theorem \ref{theo*int_parts} (Integration by Parts), which is in the Leibniz notation
\[
	\int F(x)g(x)\, dx = F(x)G(x) - \int f(x)G(x)\, dx + C.
\]
In Leibniz notation, $f = \frac{dF}{dx}$ and $g = \frac{dG}{dx}$. This leads to the notation $f\, dx = dF$ and $g\, dx = dG$, and integration by parts is sometimes written as
\[
	\int F\, dG = FG - \int G\, dF + C.
\]

\subsubsection*{Integration by Substitution}
Let $J$ be an interval and let $f:I \to J$ be a differentiable function. If $G: J \to \mathbb{R}$ is a primitie of $g$, then by the chain rule in Theorem \ref{theo*chain_rule}, $[G(f(x))]' = g(f(x)) f'(x)$ for all $x \in I$. From this it follows that
\[
	\int g(f(x)) f'(x)\, dx = G(f(x)) + C = \int g(u)\, du + C,
\]
where we used the change of variables $u = f(x)$. The substitution rule is also called \textbf{change of variable}, as one has replaced the variable $u$ in $\int g(u)\, du$ by $u=f(x)$. In Leibniz notation this is very natural: if $u = f(x)$, then $du = f'(x)\,dx$.

We also recall the second form of the substitution rule (Theorem \ref{theo*int_sub2}): if $f'\neq 0$ we can set $x = f^{-1}(u)$ so that $\frac{dx}{du} = (f^{-1})'(u)$, and obtain
\[
	\int g(f(x))\, dx = \int g(u)\frac{dx}{du}\, du + C.
\]

\subsubsection{Integration by Parts: Example}
\begin{itemize}
	\item[$\bullet$] We calculate the integral $\int \log(x)\, dx$:
	\begin{align*}
		\int \log(x)\, dx &= \int \log(x)\cdot 1\, dx\\
		&= \log(x)\cdot x - \int (\log(x))'\cdot x\, dx + C\\
		&= \log(x)\cdot x - \int \frac{1}{x}\cdot x\, dx + C\\
		&= \log(x)\cdot x - \int 1\, dx + C = \log(x)\cdot x - x + C.
	\end{align*}
	\textit{Suggestion:} To ensure that the final result is correct, one can always differentiate the result and check whether you obtain the original function. For instance, in this case, one can easily check that
	\[
	(x\log(x) - x + C)' = \log(x).
	\]
	\item[$\bullet$] We calculate the integral $\int \cos^2(x)\, dx$:
	\begin{align*}
		\int \cos^2(x)\, dx &= \cos(x)\sin(x) - \int (-\sin(x))\sin(x)\, dx + C\\
		&= \cos(x)\sin(x) + \int \underbrace{\sin^2(x)}_{\displaystyle= 1 - \cos^2(x)}\, dx + C\\
		&= \cos(x)\sin(x) + \int 1\, dx - \int\cos^2(x)\, dx + C\\
		&= \cos(x)\sin(x) + x - \int \cos^2(x)\, dx + C\\
		\Rightarrow \qquad 2\int \cos^2(x)\, dx &= \cos(x)\sin(x) + x + C\\
		\Rightarrow \qquad \int \cos^2(x) &= \frac{1}{2}(\cos(x)\sin(x) + x) + C.
	\end{align*}
\end{itemize}

\subsubsection{Integration by Substitution: Examples}
\subsubsection*{Example}
We want to compute $\int \frac{x}{1 + x^2}\, dx$. Let $u = 1 + x^2$, so that $du = 2x,\,dx$. Then we find
\[
	\int \frac{x}{1 + x^2}\, dx = \int \frac{x}{1 + x^2}\cdot \frac{du}{2x} =\frac{1}{2} \int \frac{1}{u}\, du = \frac{1}{2}\log|u| + C = \frac{1}{2}\log(1 + x^2) + C.
\]

Whenever we see the a scalar multiple of the derivative of the denominator in the numerator, we can substitute for the denominator in order to obtain a primitive of the form as above. In the above example, the numerator $x$ is a multiple of the derivative of the denominator $1 + x^2$, so we substitute for the denominator.

\subsubsection*{More Examples}
Now, we will list various examples, that require different substitutions. Note that the listed substitutions do not cover all the examples from the skript:
\begin{itemize}
	\item[$\bullet$] Let $r > 0$. For the integral $\int \sqrt{r^2 - x^2}\, dx$, we use the substitution $x = r\sin(\theta)$, $\theta \in (-\frac{\pi}{2}, \frac{\pi}{2})$. With this choice, we have $dx = r\cos(\theta) \,d\theta$, therefore
	\[
		\int\sqrt{r^2 - x^2}\, dx = \int\sqrt{r^2 - r^2\sin^2(\theta)} \cos(\theta)\, d\theta = r^2 \int \cos^2(\theta)\, d\theta.
	\]
	We can compute this integral by using integration by parts.
	\item[$\bullet$] In general, for integrals of the form $\int (a^2 - x^2)^{\frac{n}{2}}\, dx$ for $a > 0$ and $n \in \mathbb{Z}$, one uses the substitution $x = a\sin(\theta)$ with $\theta \in (-\frac{\pi}{2}, \frac{\pi}{2})$, giving $dx = a \cos(\theta)\, d\theta$ and $(a^2 - x^2)^{\frac{1}{2}} = a \cos(\theta)$.
	\item[$\bullet$] For integrals of the form $\int (a^2 + x^2)^{\frac{n}{2}}\, dx$ for $a > 0$ and $n \in \mathbb{Z}$, the substitution $x = a \tan(\theta)$ with $\theta \in (-\frac{\pi}{2}, \frac{\pi}{2})$ yields $dx = \frac{a}{\cos^2(\theta)}\,d\theta$ and $(a^2 + x^2)^{\frac{n}{2}} = \left(\frac{a}{\cos(\theta)}\right)^n$.
	\item[$\bullet$] For integrals of the form $\int x(a^2 - x^2)^{\frac{n}{2}}\, dx$ or $\int x(a^2 + x^2)^{\frac{n}{2}}\, dx$, the substitutions $u = a^2 - x^2$ or $u = a^2 + x^2$, respectively, allow us to compute the indefinite integrals.
	\item[$\bullet$] For integrals of the form $\int \sqrt{x^2 - 1}\, dx$, the substitution $x = \cosh(u)$ yields $dx = \sinh(u)\,du$ and 
	\[
		\int \sqrt{x^2 - 1}\, dx = \int \sqrt{\cosh^2(u) - 1} \sinh(u)\, du = \int \sinh^2(u)\, du.
	\]
	This indefinite integral can again be computed using integration by parts.
	\item[$\bullet$] For the integral $\int \frac{1}{\sin(x)}\, dx$, we consider the substitution $u = \tan(\frac{x}{2})$. This yields, by the doubling angle formulas for sine and cosine, that $\sin(x) = \frac{2u}{1 + u^2}$ and $dx = \frac{2}{1 + u^2}\, du$.
	\item[$\bullet$] For integrals of the form $\int\frac{1}{a^2 + x^2}\, dx$, for $a \in \mathbb{R}$, one uses the substitution $u = \frac{x}{a}$. This yields
	\begin{align*}
		\int \frac{1}{a^2 + x^2}\, dx = \int \frac{1}{a^2}\cdot\frac{1}{1 + u^2}\cdot a\, du = \frac{1}{a}\int \frac{1}{1 + u^2}\, du = \frac{1}{a}\arctan(u) + C = \frac{1}{a}\arctan\left(\frac{x}{a}\right) + C.
	\end{align*}
\end{itemize}

\subsubsection{The Gamma Function}
The \textbf{Gamma-function} $\Gamma$ is defined, for $s \in (0, \infty)$, by the improper integral
\begin{equation}
	\label{eq:gamma_func}
	\Gamma(s) = \int_{0}^{\infty}x^{s-1}e^{-x}\, dx.
\end{equation}
To verify that this improper integral indeed converges, we examine the integration limits 0 and $\infty$ separately. For $0 < a <b$ we find, using integration by parts,
\begin{equation}
	\label{eq:gamma_func_lim}
	\int_{a}^{b}x^{s-1}e^{-x}\, dx = \frac{1}{s}\big[x^se^{-x}\big]_a^b + \frac{1}{s}\int_{a}^{b} x^se^{-x}\ dx.
\end{equation}
We obtain
\begin{align*}
	\int_{0}^{b}x^{s-1}e^{-x}\, dx &= \lim_{a \to 0}\bigg(\frac{1}{s}\big[x^se^{-x}\big]_a^b + \frac{1}{s}\int_{a}^{b} x^se^{-x}\ dx\bigg)\\
	&= \frac{1}{s}b^s e^{-b} + \frac{1}{s}\int_{0}^{b}x^se^{-x}\, dx,
\end{align*}
where the integral on the right is an actual Riemann integral since the function $x^se^{-x}$ is continuous on $[0,b]$. To investigate the upper limit of integration, we note that there exists $R > 0$ such that $e^x > x^{s+2}$ holds for all $x > R$. Thus
\[
	\int_{0}^{\infty} x^se^{-x}\, dx \leq \int_{0}^{R}x^se^{-x}\, dx + \int_{R}^{\infty} x^{-2}\, dx < \infty,
\]
which shows that \eqref{eq:gamma_func_lim} converges as $b \to \infty$. Specifically, we obtain
\[
	\int_{0}^{\infty}x^{s-1}e^{-x}\, dx = \lim_{b \to \infty} \bigg(\frac{1}{s}b^s e^{-b} + \frac{1}{s}\int_{0}^{b}x^se^{-x}\, dx\bigg) = \frac{1}{s}\int_{0}^{\infty}x^se^{-x}\, dx.
\]
This shows that the gamma function satisfies the relation
\begin{equation}
	\label{eq:gamma_func_relation}
	\Gamma(s+1) = s\Gamma(s)
\end{equation}
for all $s \in (0, \infty)$, from which one can deduce that the Gamma function extends the factorial function from $\mathbb{N}$ to $(0, \infty)$. In fact
\[
	\Gamma(1) = \int_{0}^{\infty}x^0e^{-x}\, dx = \int_{0}^{\infty} e^{-x}\, dx = \big[e^{-x}\big]_0^{\infty} = e^0 = 1,
\]
therefore \eqref{eq:gamma_func_relation} implies that
\[
	\Gamma(n+1) = n\Gamma(n) = n(n-1)\Gamma(n-1) = \hdots = n!\Gamma(1) = n!\qquad \forall n \in \mathbb{N}.
\]

\subsection{Taylor Series}
As we have seen in Chapter \ref{sec:diff_calc}, given a differentiable function $f:D \to \mathbb{R}$, the derivative $f'(x_0)$ gives the slope of the tangent to the graph of $f$ at $x_0$. Moreover, the corresponding affine function
\[
	x \mapsto f(x_0) + f'(x_0)(x-x_0)
\]
approximates the function $f$ within an error $o(|x-x_0|)$ as $x \to x_0$, see Equation \eqref{eq:derivative_lil_o}. The idea behind Taylor's theorem is that the ''quality'' of the approximation can be increased by considering higher-order polynomials instead of affine approximations.

In this section, it will be convenient to use the following abuse of notation: given $a, b \in \mathbb{R}$, irrespective of the order between $a$ and $b$, $[a,b]$ denotes the interval between them. In other words, for all $a,b \in \mathbb{R}$, $[a,b]$ and $[b,a]$ denote the same interval.

We also recall that, if $a < b$, then
\[
	\bigg|\int_{a}^{b}f(x)\, dx\bigg|\leq \int_{a}^{b}|f(x)|\, dx,
\]
see Theorem \ref{theo*triangle_ineq_int}. If instead $b < a$, then a minus sign appears (recall Equation \eqref{eq:convention_int}) and we get
\[
	\bigg|\int_{a}^{b}f(x)\, dx\bigg| = \bigg|\int_{b}^{a}f(x)\, dx\bigg| \leq \int_{b}^{a}|f(x)|\, dx = -\int_{a}^{b}|f(x)|\, dx = \bigg|\int_{a}^{b}|f(x)|\, dx\bigg|.
\]
In conclusion, independently of the order of $a$ and $b$, we always have
\[
	\bigg|\int_{a}^{b}f(x)\, dx\bigg|\leq \bigg|\int_{a}^{b}|f(x)|\, dx\bigg|.
\]

Let $I \subseteq \mathbb{R}$ be an open interval, and $f:I\to \mathbb{R}$ be an $n$-times differentiable function. The $n$-th \textbf{Taylor approximation} of $f$ around a point $x_0 \in I$ is the polynomial function
\begin{equation}
	\label{eq:taylor_poly}
	P_n(x) = \sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x- x_0)^k.
\end{equation} 
Note that, with this choice of the coefficients, $P^{(k)}(x_0) = f^{(k)}(x_0)$ for $k \in \{0,\hdots, n\}$.

We will state and prove different versions of Taylor's Theorem. We begin with this first version:

\begin{theorem}{Taylor Expansion to Order $n$ with Integral Remainder}{taylor_int_remainder}
	Let $n \geq 1$, $f:[a,b]\to \mathbb{R}$ be an $n$-times continuously differentiable function, and fix $x_0\in [a,b]$. Then, for all $x \in [a,b]$,
	\begin{equation}
		\label{eq:taylor_int_remainder}
		f(x) = P_{n-1}(x) + \int_{x_0}^{x}f^{(n)}(t)\frac{(x-t)^{n-1}}{(n-1)!}\, dt,
	\end{equation}
	where $P_{n-1}$ is the $(n-1)$-th Taylor approximation of $f$ defined in Equation \eqref{eq:taylor_poly}.
\end{theorem}
\begin{remark}
	In the above theorem, the assumption that $f$ is an $n$-times continuously differentiable function guarantees that the integral of the continuous function $t \mapsto f^{(n)}(t)\frac{(x-t)^{n-1}}{(n-1)!}$ exists.
\end{remark}

\begin{proof}
	The proof follows by induction on $n$ and integration by parts.
	
	If $n=1$ then $f:[a,b]\to \mathbb{R}$ is continuously differentiable and, by Corollary \ref{cor*int_vs_deriv}, we get
	\[
		f(x) = f(x_0) + \int_{x_0}^{x}f'(t)\, dt = P_0(x) + \int_{x_0}^{x} f^{(1)}(t)\frac{(x-t)^0}{0!}\, dt,
	\]
	as desired.
	
	To explain the idea behind the inductive step, assume first $n=2$ (so $f$ is twice continuously differentiable). Then, in the integral above, we can apply integration by parts to the functions $f'(t)$ and $g(t) = t-x$. Indeed, since $g' = 1$ and $g(x) = 0$, we get
	\begin{align*}
		f(x) &= f(x_0) + \int_{x_0}^{x}f'(t)g'(t)\, dt\\
		&= f(x_0) + \big[f'(t)g(t)\big]_{x_0}^x - \int_{x_0}^{x}f''t(t)g(t)\, dt\\
		&= f(x_0) + f'(x_0)(x-x_0) + \int_{x_0}^{x}f''(t)(x-t)\, dt\\
		&= P_1(x) + \int_{x_0}^{x}f^{(2)}(t)\frac{(x-t)^1}{1!}\, dt.
	\end{align*}
	This proves the case $n=2$.
	
	More generally, assume that the statement of the theorem is true for some $n \geq 1$ and that $f:[a,b]\to \mathbb{R}$ is an $(n+1)$-times continuously differentiable function. Then, by induction hypothesis,
	\[
		f(x) = \sum_{k=0}^{n-1}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + \int_{x_0}^{x}f^{(n)}(t)\frac{(x-t)^{n-1}}{(n-1)!}\, dt
	\]
	for all $x \in [a,b]$. If we set $g(t) = -\frac{(x-t)^n}{n!}$, then $g'(t) = \frac{(x-t)^{n-1}}{(n-1)!}$ and it follows from integration by parts that
	\begin{align*}
		f(x) &= \sum_{k=0}^{n-1}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + \bigg[f^{(n)}(t)g(t)\bigg]_{x_0}^x - \int_{x_0}^{x}f^{(n+1)}(t)g(t)\, dt\\
		&= \sum_{k=0}^{n-1}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + f^{(n)}(x_0)\frac{(x-x_0)^{n}}{n!} + \int_{x_0}^{x}f^{(n+1)}(t)\frac{(x-t)^n}{n!}\, dt\\
		&= \sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + \int_{x_0}^{x}f^{(n+1)}(t)\frac{(x-t)^n}{n!}\, dt.
 	\end{align*}
 	This proves the induction step, and hence the result.\qedhere
\end{proof}

Next, we prove the following alternative version of Taylor's Theorem. Here, we do not require $f^{(n)}$ to be continuous, but only to exist. Note that in the case $n=1$, this result corresponds to the Mean Value Theorem \ref{theo*mean_val_theo}.

\begin{theorem}{Taylor Expansion to Order $n$ with Lagrange Remainder}{taylor_lagrange_remainder}
	Let $n\geq 1$, $f:[a,b]\to \mathbb{R}$ be $n$-times differentiable function, and fix $x_0 \in [a,b]$. Then, for all $x \in [a,b]$ with $x\neq x_0$ there exists $\xi_L\in(x_0,x)$ such that
	\begin{equation}
		\label{eq:taylor_lagrange_remainder}
		f(x) = P_{n-1}(x) + \frac{f^{(n)}(\xi_L)}{n!}(x-x_0)^n.
	\end{equation}
\end{theorem}
\begin{remark}
	If $x = x_0$, the formula holds trivially: one may take $\xi_L = x_0$, since 
	\[
		f(x_0) = P_{n-1}(x_0) \qquad \text{and} \qquad \frac{f^{(n)}(\xi_L)}{n!}(x-x_0)^n = 0.
	\]
\end{remark}

\begin{proof}
	Fix $x \in [a,b]$ (in the skript it's the open interval $(a,b)$, while in the lecture we used the closed interval $[a,b]$). W.l.o.g, assume $x > x_0$ (the case $x < x_0$ is analogous) and consider the function $g:(a,b)\to \mathbb{R}$ defined as
	\begin{equation}
		\label{eq:lagrange_remainder_help_func}
		g(t) = f(t) + f^{(1)}(t)(x-t) + \; \hdots \; + \frac{f^{(n-1)}(t)}{(n-1)!}(x-t)^{n-1} = \sum_{k=0}^{n-1}\frac{f^{(k)}(t)}{k!}(x-t)^k.
	\end{equation}
	Then $g(x) = f(x)$ and $g(x_0) = P_{n-1}(x_0)$. Also, its derivative is given by
	\begin{align*}
		g'(t) &= \sum_{k=0}^{n-1} \frac{f^{(k+1)}(t)}{k!}(x-t)^k - \sum_{k=0}^{n-1}\frac{f^{(k)}(t)}{k!}k(x-t)^{k-1}\\
		&= \sum_{k=0}^{n-1} \frac{f^{(k+1)}(t)}{k!}(x-t)^k - \sum_{k=0}^{n-1}\frac{f^{(k)}(t)}{(k-1)!}(x-t)^{k-1}\\
		&= \sum_{k=0}^{n-1} \frac{f^{(k+1)}(t)}{k!}(x-t)^k - \sum_{k=0}^{n-2}\frac{f^{(k+1)}(t)}{k!}(x-t)^{k} = \frac{f^{(n)}(t)}{(n-1)!}(x-t)^{n-1}.
	\end{align*}
	Hence, applying the Cauchy Mean Value Theorem \ref{theo*cauchy_mean_val} in the interval $[x_0, x]$ to the functions $g(t)$ and $h(t) = -(x-t)^n$, we deduce the existence of a point $\xi_L \in (x_0, x)$ such that
	\[
		\frac{f(x) - P_{n-1}(x)}{(x-x_0)^n} = \frac{g(x) - g(x_0)}{h(x) - h(x_0)} = \frac{g'(\xi_L)}{h'(\xi_L)} = \frac{\frac{f^{(n)}(\xi_L)}{(n-1)!}(x-\xi_L)^{n-1}}{n(x-\xi_L)^{n-1}} = \frac{f^{(n)}(\xi_L)}{n!}.
	\]
	This implies Equation \eqref{eq:taylor_lagrange_remainder} and conclude the proof. \qedhere
\end{proof}

We can now state out two versions of Taylor's approximation, using the little-o and the big-O notation.

\begin{corollary}{Taylor Approximation with Little-o}{taylor_lil_o}
	Let $n\geq 1$, $f:[a,b]\to \mathbb{R}$ be an $n$-times continuously differentiable function, and fix $x_0\in [a,b]$. Then, for all $x \in [a,b]$,
	\begin{equation}
		\label{eq:taylor_lil_o}
		f(x) = P_n(x) + o(|x-x_0|^n) \qquad \text{as }x\to x_0.
	\end{equation}
\end{corollary}

\begin{proof}
	Thanks to Theorem \ref{theo*taylor_int_remainder}, we have that
	\begin{align*}
		f(x) &= P_{n-1}(x) + \int_{x_0}^{x}f^{(n)}(t) \frac{(x-t)^{n-1}}{(n-1)!}\, dt\\
		&= P_{n-1}(x) + \int_{x_0}^{x}f^{(n)}(x_0)\frac{(x-t)^{n-1}}{(n-1)!} \, dt + \int_{x_0}^{x}(f^{(n)}(t) - f^{(n)}(x_0))\frac{(x-t)^{n-1}}{(n-1)!}\, dt.
	\end{align*}
	Also, using the change of variable $s = x-t$, we note that
	\begin{equation}
		\label{eq:int_taylor_lil_o}
		\int_{x_0}^{x}\frac{(x-t)^{n-1}}{(n-1)!}\, dt = \frac{1}{(n-1)!}\int_{0}^{x -x_0} s^{n-1}\, ds = \frac{1}{(n-1)!}\frac{(x-x_0)^n}{n} = \frac{(x-x_0)^n}{n!},
	\end{equation}
	therefore
	\[
		\int_{x_0}^{x} f^{(n)}(x_0)\frac{(x-t)^{n-1}}{(n-1)!} \, dt = f^{(n)}(x_0)\frac{(x-x_0)^n}{n!}.
	\]
	Hence, we can write
	\begin{equation}
		\label{eq:taylor_lil_o2}
		\begin{aligned}
			f(x) &= P_{n-1}(x) + f^{(n)}(x_0)\frac{(x-x_0)^n}{n!} + \int_{x_0}^{x}(f^{(n)}(t) - f^{(n)}(x_0))\frac{(x-t)^{n-1}}{(n-1)!}\, dt\\
			&= P_n(x) + \int_{x_0}^{x}(f^{(n)}(t) - f^{(n)}(x_0))\frac{(x-t)^{n-1}}{(n-1)!}\, dt.
		\end{aligned}
	\end{equation}
	Now, given $\varepsilon > 0$, it follows from the continuity of $f^{(n)}$ at $x_0$ that there exists $\delta > 0$ such that $|f^{(n)}(x) - f^{(n)}(x_0)| < \varepsilon$ for all $x \in (x_0 - \delta, x_0 + \delta) \cap [a,b]$. Therefore, if $x \in (x_0 - \delta, x_0 + \delta) \cap [a,b]$, we can bound the integrand in the last integral by
	\begin{align*}
				\bigg|(f^{(n)}(t) - f^{(n)}(x_0))\frac{(x-t)^{n-1}}{(n-1)!}\bigg| &\leq |f^{(n)}(t) - f^{(n)}(x_0)|\frac{|x-t|^{n-1}}{(n-1)!}\\
				&< \varepsilon \frac{|x-t|^{n-1}}{(n-1)!} \qquad \forall t \in [x_0,x].
	\end{align*}
	Hence, using Equation \eqref{eq:int_taylor_lil_o} again, we get
	\begin{align*}
		|f(x) - P_n(x)| &\leq \bigg|\int_{x_0}^{x}(f^{(n)}(t) - f^{(n)}(x_0))\frac{(x-t)^{n-1}}{(n-1)!}\, dt \bigg|\\
		&< \varepsilon \bigg|\int_{x_0}^{x} \frac{|x-t|^{n-1}}{(n-1)!}\, dt\bigg| = \varepsilon\frac{|x-x_0|^n}{n!} \leq \varepsilon |x-x_0|^n,
	\end{align*}
	which shows that $f(x) - P_n(x) = o(|x-x_0|^n)$ as $x\to x_0$.\qedhere
\end{proof}

\begin{corollary}{Taylor Approximation with Big-O}{taylor_big_o}
	Let $n \geq 1$, $f:[a,b]\to \mathbb{R}$ be an $n$-times differentiable function, and fix $x_0 \in [a,b]$. Assume that there exists $M > 0$ such that $|f^{(n)}(x)| \leq M$ for all $x \in [a,b]$. Then
	\begin{equation}
		\label{eq:taylor_big_o}
		f(x) = P_{n-1}(x) + O(|x-x_0|^n)\qquad \text{as }x\to x_0.
	\end{equation}
\end{corollary}

\begin{proof}
	Given $x \in [a,b]$, we apply Equation \eqref{eq:taylor_lagrange_remainder} to find a point $\xi_L \in [x_0,x]$ such that
	\[
		f(x) - P_{n-1}(x) = \frac{f^{(n)}(\xi_L)}{n!}(x-x_0)^n.
	\]
	Since $|f^{(n)}(\xi_L)| \leq M$, this implies that
	\[
		|f(x) - P_{n-1}(x)| \leq \frac{M}{n!}|x-x_0|^n \leq M |x-x_0|^n,
	\]
	therefore $f(x) - P_{n-1}(x) = O(|x-x_0|^n)$ as $x \to x_0$, as desired.\qedhere
\end{proof}

\subsubsection*{Example}
As mentioned earlier, if $f$ is differentiable at $x_0$, then
\[
	f(x) = f(x_0) + f'(x_0)(x-x_0) + o(|x-x_0|) \qquad \text{as }x \to x_0.
\]
Taylor's Theorem allows us to obtain more accurate approximations when $f$ has higher regularity.
\begin{enumerate}
	\item If $f$ is twice differentiable and $f''$ is bounded in a neighborhood of $x_0$, then Corollary \ref{cor*taylor_big_o} (with $n=2$) gives
	\[
		f(x) = f(x_0) + f'(x_0)(x-x_0) + O(|x-x_0|^2) \qquad \text{as }x \to x_0.
	\]
	If in addition $f''$ is continuous (equivalently , $f \in C^2$), then Corollary \ref{cor*taylor_lil_o} yields
	\[
		f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + o(|x-x_0|^2) \qquad \text{as }x \to x_0.
	\]
	\item If $f$ is smooth, then Corollary \ref{cor*taylor_lil_o} yields
	\[
		f(x) = P_n(x) + o(|x-x_0|^n) \qquad \text{as }x \to x_0,
	\]
	while Corollary \ref{cor*taylor_big_o} applied with $n+1$ in place of $n$ gives
	\[
		f(x) = P_n(x) + O(|x-x_0|^{n+1}) \qquad \text{as }x \to x_0.
	\]
	Hence, in the case when $f$ is smooth, the bound on $f - P_n$ provided by Corollary \ref{cor*taylor_big_o} is often more convenient.
\end{enumerate}

\subsubsection*{Example}
We illustrate how Taylor expansions can be used to compute limits of more complicated functions

To compute the limit $\lim_{x \to 0} \frac{\sin(x) - x}{x^3}$, we consider the Taylor polynomial of $\sin$ at 0 of degree 3. Since
\[
	\sin(0) = 0, \qquad \sin'(0) = 1, \qquad \sin''(0) = 0,\qquad \sin^{(3)}(0) = -1,
\]
the Taylor polynomial of degree 3 at 0 is
\[
	P_3(x) = 0 + 1\cdot x + \frac{0}{2}\cdot x^2 + \frac{-1}{6}\cdot x^3 = x - \frac{x^3}{6}.
\]
Taylor's theorem gives
\[
	\sin(x) = P_3(x) + O(x^4) = x - \frac{x^3}{6} + O(x^4) \qquad \text{as }x \to 0.
\]
We stop the expansion at order 3 because the limit involves $x^3$ in the denominator; any term of order $\geq 4$ will disappear in the limit.
Then
\[
	\sin(x) - x = \big(x - \frac{x^3}{6} + O(x^4)\big) - x = -\frac{x^3}{6} + O(x^4) \qquad \text{as }x \to 0,
\]
and thus
\[
	\frac{\sin(x) - x}{x^3} = \frac{-\frac{x^3}{6} + O(x^4)}{x^3} = -\frac{1}{6} + O(x^4) \qquad \text{as }x \to 0.
\]
Hence,
\[
	\lim_{x \to 0} \frac{\sin(x) - x}{x^3} = -\frac{1}{6}.
\]


\begin{remark}
	In the previous example we showed that
	\[
		\sin(x) = x - \frac{x^3}{6} + O(x^4) \qquad \text{as }x \to 0.
	\]
	These coefficients agree with those appearing in the power series expansion of $\sin$. In Section \ref{sec:analytic_func} below, we will see that $\sin$ is an \textit{analytic} function, meaning that its Taylor series around 0 actually converges to $\sin(x)$. For such functions, the Taylor polynomial of degree $n$ is simply the truncation of an infinite series that represents the function itself.
\end{remark}

\subsubsection*{Example}
We can use the Taylor approximation to refine the discussion in Section \ref{sec:local_extrema}. Let $f:(a,b) \to \mathbb{R}$ be an $n$-times continuously differentiable function. Suppose $x_0 \in (a,b)$ satisfies
\[
	f'(x_0) = \; \hdots \; = f^{(n-1)}(x_0) = 0.
\]
Then the following implications hold:
\begin{itemize}
	\item[$\bullet$] if $f^{(n)}(x_0) < 0$ and $n$ is even, then $f$ has an isolated local maximum at $x_0$.
	\item[$\bullet$] if $f^{(n)}(x_0) > 0$ and $n$ is even, then $f$ has an isolated local minimum at $x_0$.
	\item[$\bullet$] if $f^{(n)}(x_0) \neq 0$ and $n$ is odd, then $x_0$ is not a local extremum of $f$.
\end{itemize}
All three statements follow from Equation \eqref{eq:taylor_lagrange_remainder}, which, in this case, takes the form
\[
	f(x) = f(x_0) + \frac{f^{(n)}(\xi_L)}{n!}(x-x_0)^n, \qquad \xi_L \in (x_0,x).
\]
Indeed, if $f^{(n)}(x_0) > 0$, by continuity there exists $\delta > 0$ such that $f^{(n)}(\xi_L) > 0$ for $\xi_L \in (x_0,x) \subseteq (x_0-\delta, x_0+\delta)$. If $n$ is even, then $(x-x_0)^n > 0$ for $x \neq x_0$ and we deduce that $f(x) > f(x_0)$ for $x \in (x_0 - \delta, x_0 + \delta)$ with $x \neq x_0$. When $n$ is odd, then $(x-x_0)^n$ changes sign when considering $x > x_0$ and $x < x_0$, so $x_0$ is not a local extremum of $f$.

On the other hand, if $f^{(n)}(x_0) < 0$ and $n$ is even, the same argument as above shows that $f(x) < f(x_0)$ for $x \in (x_0-\delta, x_0+\delta)$ with $x \neq x_0$, while the case $n$ odd yields that $x_0$ is not a local extremum of $f$.


\subsubsection{Analytic Functions}
\label{sec:analytic_func}
Motivated by Taylor's theorem, one might expect that if $f$ is smooth and we replace the finite Taylor polynomial $P_n$ by the full \textbf{Taylor series}
\[
	\sum_{k=0}^{\infty}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k,
\]
then this infinite series should converge to $f(x)$. Unfortunately, this is not true in general. Only rather special smooth functions enjoy this property.

Note that the Taylor series is centered at $x_0$ instead of 0 (i.e., $x^n$ is replaced with $(x-x_0)^n$). Hence, all theorems about power series from Section \ref{sec:pwr_series} still hold, but taking into account that now $x_0$ plays the role of the center. In particular, if the series has radius of convergence $R > 0$, then it converges for all $x \in (x_0 - R, x_0 + R)$, while it diverges for $|x-x_0| > R$.

\begin{definition}{Analytic Functions}{analytic_func}
	Let $I \subseteq \mathbb{R}$ be an interval and $x_0 \in I$. A smooth function $f:I \to \mathbb{R}$ is called \textbf{analytic} at $x_0$ if the Taylor series of $f$ around $x_0$ has radius of convergence $R > 0$ and there exists $\delta \in (0, R)$ such that
	\[
		f(x) = \sum_{n=0}^{\infty}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \qquad \forall x \in (x_0 - \delta, x_0 + \delta) \cap I.
	\]
	We say $f$ is analytic in $I$ if $f$ is analytic at all points in $I$.
\end{definition}

In other words, analytic functions $f:I \to \mathbb{R}$ are characterized by the fact that, for every point $x_0 \in I$, there exists a power series that converges to $f$ in a neighborhood of $x_0$.

As the next example shows, there are smooth functions $f$ whole Taylor series converges to a function different form $f$.

\subsubsection*{Example}
Consider the function $\psi:\mathbb{R} \to \mathbb{R}$ defined by
\[
	\psi:x\in \mathbb{R} \mapsto \begin{cases}
		\exp\bigg(-\dfrac{1}{x}\bigg) \quad &\text{if }x > 0,\\
		0 \quad &\text{if } x \leq 0
	\end{cases}.
\]
We note that $\psi$ is smooth on $\mathbb{R}$ and satisfies $\psi^{(n)}(0) = 0$ for all $n \in \mathbb{N}$. Hence, the Taylor series of the function $\psi$ at the point $x_0 = 0$ is the zero series, i.e.,
\[
	\sum_{n=0}^{\infty}\frac{\psi^{(n)}(0)}{n!}x^n = \sum_{n=0}^{\infty}\frac{0}{n!}x^n = 0.
\]
The series has an infinite radius of convergence and converges to the function 0. Since $\psi(x) > 0$ holds for all $x > 0$, the Taylor series does not converge to $\psi$ in a neighborhood of $x_0 = 0$, and so $\psi$ is not analytic at the point $x_0 = 0$.

The next result provides a criterion that guarantees that the Taylor series of $f$ converges to $f$ in a neighborhood of $x_0$.

\begin{theorem}{A Criterion for Analyticity at $x_0$}{crit_analytic}
	Let $I \subseteq \mathbb{R}$ be an interval and $f:I \to \mathbb{R}$ a smooth function. Given $x_0 \in I$, assume that there exists constants $r, C_0, A > 0$ such that
	\[
		|f^{(n)}(x)| \leq C_0 A^n n! \qquad \forall x \in (x_0 - r, x_0 + r)\cap I, \; \forall n \in \mathbb{N}.
	\]
	Then $f$ is analytic at $x_0$.
\end{theorem}

\begin{proof}
	Let $I \subseteq \mathbb{R}$ and fix $x_0 \in I$. We first estimate the radius of convergence $R$ of the Taylor series. If we define $a_n = \frac{f^{(n)}(x_0)}{n!}$, then the Taylor series is equal to $\sum_{n=0}^{\infty} a_n (x-x_0)^n$. Thus, thanks to our assumption on the size of $|f^{(n)}|$, it follows that
	\[
		|a_n| \leq \frac{C_0 A^n n!}{n!} = C_0 A^n.
	\]
	This implies that
	\[
		\limsup_{n \to \infty} \sqrt[n]{|a_n|} \leq \limsup_{n \to \infty}\sqrt[n]{C_0A^n} = \underbrace{\limsup_{n \to \infty} \sqrt[n]{C_0}}_{\displaystyle = 1} A = A,
	\]
	therefore, by the definition of radius of convergence (see Definition \ref{def*radius_conv}), $R \geq \frac{1}{A}$.
	
	Now, we want to show that there exists a $\delta \in (0, R)$ such that every the Taylor series of $f(x)$ at $x_0$ converges to $f(x)$ for every $x \in (x_0 - \delta, x_0 + \delta) \cap I$.
	So, fix $\delta < \min\{r, \frac{1}{A}\}$. Given $x \in (x_0 - \delta, x_0 + \delta) \cap I$, we apply Equation \eqref{eq:taylor_lagrange_remainder} to find $\xi_L \in (x_0, x)$, and our assumption on the size of $|f^{(n)}|$ to deduce that
	\[
		|f(x) - P_{n-1}(x)| \leq \frac{|f^{(n)}(\xi_L)|}{n!}|x-x_0|^n \leq C_0 A^n |x-x_0|^n \leq C_0 (A\delta)^n.
	\]
	Since $A\delta < 1$ (because $\delta < \frac{1}{A}$), letting $n \to \infty$ we conclude that
	\[
		f(x) = \lim_{n \to \infty} P_{n-1}(x) = \sum_{n=0}^{\infty}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \qquad \forall x \in (x_0- \delta, x_0+\delta)\cap I,
	\]
	as desired.\qedhere
\end{proof}

As a direct consequence of Theorem \ref{theo*crit_analytic}, we immediately deduce the following:

\begin{corollary}{A Criterion for Analyticity}{crit_analytic2}
	Let $f:[a,b] \to \mathbb{R}$ be a smooth function, and assume there exists constants $C_0,A > 0$ such that
	\[
		|f^{(n)}(x)| \leq C_0 A^n n! \qquad \forall x \in [a,b], \;\forall n \in \mathbb{N}.
	\]
	Then $f$ is analytic on $[a,b]$.
\end{corollary}
