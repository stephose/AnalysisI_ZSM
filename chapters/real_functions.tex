%
% (c) 2025 Autor, ETH ZÃ¼rich
%
% !TEX root = main.tex
% !TEX encoding = UTF-8
%

\section{Functions of one Real Variable}
In this chapter we study real-valued functions defined on subsets of $\mathbb{R}$, typically intervals. The central concept is \textit{continuity}.

\subsection{Real valued functions}

\subsubsection{Boundedness and Monotonitcity}
For a non-empty set $D \subseteq \mathbb{R}$, the set of \textbf{real-valued} functions on $D$ is
\[
	\mathcal{F}(D) = \{f\;|\;f:D \to \mathbb{R}\}.
\]
For $f_1, f_2 \in \mathcal{F}(D)$, $\alpha \in \mathbb{R}$, and $x \in D$ we define
\[
	(f_1 + f_2)(x) = f_1(x) + f_2(x), \qquad (\alpha f_1)(x) = \alpha f_1(x), \qquad (f_1f_2)(x) = f_1(x)f_2(x).
\]
Given $\alpha \in \mathbb{R}$, we write $f \equiv \alpha$ for the constant function $x \mapsto \alpha$ on $D$.

\begin{remark}
	With the operations above, $\mathcal{F}(D)$ is a commutative ring (the additive identity is $f \equiv 0$ and the multiplicative identity is $f \equiv 1$).
\end{remark}

A point $x \in D$ is a \textbf{zero} of $f \in \mathcal{F}(D)$ if $f(x) = 0$. The \textbf{zero set} of $f$ is $\{x \in D\;|\; f(x) = 0\}$.
We order $\mathcal{F}(D)$ pointwise: for $f_1, f_2 \in \mathcal{F}(D)$,
\begin{align*}
	f_1 \leq f_2 \quad &\Leftrightarrow \quad f_1(x) \leq f_2(x) \quad \forall x \in D,\\
	f_1 < f_2 \quad &\Leftrightarrow \quad f_1(x) < f_2(x) \quad \forall x \in D.
\end{align*}
We say that $f \in \mathcal{F}(D)$ is \textbf{non-negative} if $f \geq 0$, and \textbf{positive} if $f > 0$.

\begin{definition}{Bounded Functions}{bounded_func}
	Let $D \neq \emptyset$ and $f:D \to \mathbb{R}$. We say that $f$ is \textbf{bounded from above} if there exists $M > 0$ such that
	\[
		f(x) \leq M \qquad \forall x \in D.
	\]
	We say that $f$ is \textbf{bounded from below} if there exists $M > 0$ such that
	\[
		f(x) \geq -M \qquad \forall x \in D.
	\]
	We say that $f$ is \textbf{bounded} if it is both bounded from above and from below. Equivalently, $f$ if bounded if there exists $M > 0$ such that
	\[
		|f(x)| \leq M \qquad \forall x \in D.
	\]
\end{definition}

\begin{definition}{Monotone Functions}{monotone_func}
	Let $D \subseteq \mathbb{R}$ and $f:D \to \mathbb{R}$. The function $f$ is:
	\begin{enumerate}
		\item \textbf{increasing} if $x < y \quad \Rightarrow \quad f(x) \leq f(y) \quad \forall x,y \in D$;
		\item \textbf{strictly increasing} if $x < y \quad \Rightarrow \quad f(x) < f(y) \quad \forall x,y \in D$;
		\item \textbf{decreasing} if $x < y \quad \Rightarrow \quad f(x) \geq f(y) \quad \forall x,y \in D$;
		\item \textbf{strictly decreasing} if $x < y \quad \Rightarrow \quad f(x) > f(y) \quad \forall x,y \in D$.
	\end{enumerate}
	We call $f$ \textbf{monotone} if it is increasing or decreasing, and \textbf{strictly monotone} if it is strictly increasing or strictly decreasing.
\end{definition}

\subsubsection{Continuity}

\begin{definition}{Continuous Functions}{cont_func}
	Let $D \subseteq \mathbb{R}$ and $f:D \to \mathbb{R}$. We say that $f$ is \textbf{continuous at} $x_0 \in D$ if for all $\varepsilon > 0$ there exists $\delta > 0$ such that
	\[
		\forall x \in D, \quad |x - x_0| < \delta \quad \Rightarrow \quad |f(x) - f(x_0)| < \varepsilon.
	\]
	We say that $f$ is \textbf{continuous on} $D$ if it is continuous at every point of $D$.
\end{definition}

\begin{remark}
	It suffices to verify the implication above for small $\varepsilon$. Precisely, assume there exists $\varepsilon_0 > 0$ such that for every $\varepsilon \in (0, \varepsilon_0]$ there is a $\delta > 0$ such that
	\[
		\forall x \in D, \quad |x - x_0| < \delta \quad \Rightarrow \quad |f(x) - f(x_0)| < \varepsilon.
	\]
	Then $f$ is continuous at $x_0$.
	
	Indeed, for $\varepsilon_0 > \varepsilon$ we can choose the number $\delta > 0$ corresponding to $\varepsilon$ to get
	\[
		\forall x \in D, \quad |x - x_0| < \delta \quad  \Rightarrow \quad |f(x) - f(x_0)| < \varepsilon < \varepsilon_0.
	\]
	In other words, if $\delta$ works for $\varepsilon$, then it works for all $\varepsilon_0 > \varepsilon$.
\end{remark}

\begin{definition}{Restriction}{restriction}
	Let $D \subseteq \mathbb{R}$ and $f:D \to \mathbb{R}$. For any $D' \subseteq D$ the \textbf{restriction} of $f$ to $D'$ is the function $f|_{D'}:D' \to \mathbb{R}$ defined by
	\[
		f|_{D'}(x) = f(x) \qquad \forall x \in D'.
	\]
	We regard $f|_{D'}$ and $f$ as different functions unless $D' = D$.
\end{definition}

\begin{proposition}{Combination of Continuous Functions}{comb_cont_func}
	Let $D \subseteq \mathbb{R}$, and let $f_1, f_2:D \to \mathbb{R}$ be continuous at $x_0 \in D$. Then $f_1 + f_2$, $f_1f_2$, and $\alpha f_1$ (for any $\alpha \in \mathbb{R}$) are continuous at $x_0$.
\end{proposition}

\begin{proof}
	We first prove the result for the sum. Let $\varepsilon > 0$. Since $f_1$ and $f_2$ are continuous at $x_0$, there exists $\delta_1, \delta_2 > 0$ such that for all $x \in D$,
	\[
		|x - x_0| < \delta_1 \; \Rightarrow \; |f_1(x) - f_1(x_0)| < \frac{\varepsilon}{2}, \quad |x - x_0| < \delta_2 \; \Rightarrow \; |f_2(x) - f_2(x_0)| < \frac{\varepsilon}{2}.
	\]
	So, choosing $\delta = \min{\delta_1, \delta_2}$, for $|x - x_0| < \delta$ we get
	\[
		|(f_1 + f_2)(x) - (f_1 + f_2)(x_0)| \leq |f_1(x) - f_1(x_0)| + |f_2(x) - f_2(x_0)| < \varepsilon,
	\]
	which shows that $f_1 + f_2$ is continuous at $x_0$.
	
	For the product, note that
	\begin{align*}
		|f_1(x)f_2(x) - f_1(x_0)f_2(x_0)| &= |f_1(x)f_2(x) - f_1(x_0)f_2(x) + f_1(x_0)f_2(x) - f_1(x_0)f_2(x_0)|\\
		&\leq |f_1(x)f_2(x) - f_1(x_0)f_2(x)| + |f_1(x_0)f_2(x) - f_1(x_0)f_2(x_0)|\\
		&= |f_2(x)||f_1(x) - f_1(x_0)| + |f_1(x_0)||f_2(x) - f_2(x_0)|.
	\end{align*}
	Now, first choose $\delta_0 > 0$ such that $|x - x_0| < \delta_0$ implies $|f_2(x) - f_2(x_0)| < 1$, so that
	\[
		|x - x_0| < \delta_0 \quad \Rightarrow \quad |f_2(x)| < 1 + |f_2(x_0)|.
	\]
	Then choose $\delta_1, \delta_2 > 0$ such that
	\begin{align*}
		|x - x_0| < \delta_1 \quad \Rightarrow \quad |f_1(x) - f_1(x_0)| < \frac{\varepsilon}{2 (1 + |f_2(x_0)|)},\\
		|x - x_0| < \delta_2 \quad \Rightarrow \quad |f_2(x) - f_2(x_0)| < \frac{\varepsilon}{2 (1 + |f_1(x_0)|)}.
	\end{align*}
	So choosing $\delta = \min{\delta_0, \delta_1, \delta_2}$, for $|x - x_0| < \delta$ we get
	\begin{align*}
		|f_1(x)f_2(x) - f_1(x_0)f_2(x_0)| &< |f_2(X)|\, \frac{\varepsilon}{2 (1 + |f_2(x_0)|)} + |f_1(x_0)|\, \frac{\varepsilon}{2 (1 + |f_1(x_0)|)}\\
		&< (1 + |f_2(x_0)|)\, \frac{\varepsilon}{2 (1 + |f_2(x_0)|)} + |f_1(x_0)|\, \frac{\varepsilon}{2 (1 + |f_1(x_0)|)}\\
		&< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon,
	\end{align*}
	thus $f_1f_2$ is continuous at $x_0$.
	
	Finally, the statement about $\alpha f_1$ follows by choosing $f_2 \equiv \alpha$ (a constant function) and using the product case proved above: since $f_1$ and $f_2$ are continuous at $x_0$, their product $f_1f_2 = \alpha f_1$ is continuous at $x_0$. \qedhere
\end{proof}

\begin{definition}{Sum and Product Notation}{sum_prod_notation}
	Let $n \in \mathbb{N}$ and $a_0, a_1, \hdots , a_n \in \mathbb{R}$. We use the notation
	\[
		\sum_{j=0}^{n} a_j = a_0 + a_1 + \hdots + a_n, \qquad \prod_{j=0}^{n} a_0 \cdot a_1 \cdot \hdots \cdot a_n.
	\]
	Here $a_j$ is a \textbf{summand} in the sum and a \textbf{factor} in the product; $j$ is the \textbf{index} (or \textbf{running variable}).
	If $J$ is a finite set and numbers $(a_j)_{j\in J}$ are given, we write
	\[
		\sum_{j \in J} a_j, \qquad \prod_{j\in J} a_j.
	\]
	By convention, for the empty index set $\emptyset$,
	\[
		\sum_{j\in \emptyset} a_j = 0, \qquad \prod_{j\in \emptyset} a_j = 1.
	\]
\end{definition}

\begin{proposition}{Composition of Continuous Functions}{comp_cont_func}
	Let $D_1, D_2 \subseteq \mathbb{R}, x_0 \in D_1$ and $f:D_1 \to D_2$ be continuous at $x_0$. If $g:D_2 \to \mathbb{R}$ is continuous at $f(x_0)$, then $g \circ f:D_1 \to \mathbb{R}$ is continuous at $x_0$. In particular, the composition of continuous functions is continuous.
\end{proposition}

\begin{proof}
	Let $\varepsilon > 0$. By continuity of $g$ at $f(x_0)$, there exists $\eta > 0$ such that
	\[
		\forall y \in D_2, \quad |y - f(x_0)| < \eta \quad \Rightarrow \quad |g(y) - g(f(x_0))| < \varepsilon.
	\]
	By continuity of $f$ at $x_0$, there exists $\delta > 0$ such that
	\[
		\forall x \in D_1, \quad |x - x_0| < \delta \quad \Rightarrow \quad |f(x) - f(x_0)| < \eta.
	\]
	Combining the implications gives, for any $x \in D_1$,
	\[
		|x - x_0| < \delta \quad \Rightarrow \quad |f(x) - f(x_0)| < \eta \quad \Rightarrow \quad |g(f(x)) - g(f(x_0))| < \varepsilon. \qedhere
	\]
\end{proof}

\begin{remark}
	\label{rmk:modulus_cont}
	Applying Proposition \ref{prop*comp_cont_func} with $g(y) = |y|$, we see that if $f:D \to \mathbb{R}$ is continuous, then $x \mapsto |f(x)|$ is continuous.
\end{remark}

\subsubsection{Sequential Continuity}

\begin{definition}{Notation for Limits of Sequences}{notation_lim_seq}
	Let $(x_n)_{n=0}^{\infty} \subseteq \mathbb{R}$ and $\overline{x} \in \mathbb{R}$. We write
	\[
		x_n \to \bar{x} \qquad \text{or} \qquad x_n \underset{n \to \infty}{\longrightarrow} \bar{x}
	\] 
	to mean
	\[
		\lim_{n \to \infty} x_n = \bar{x}.
	\]
\end{definition}

\begin{theorem}{Continuity = Sequential Continuity}{cont_seq_cont}
	Let $D \subseteq \mathbb{R}$, $f:D \to \mathbb{R}$, and $\bar{x} \in D$. Then $f$ is continuous at $\bar{x}$ if and only if for every sequence $(x_n)_{n=0}^{\infty} \subseteq D$ with $x_n \to \bar{x}$ we have $f(x_n) \to f(\bar{x})$.  
\end{theorem}

\begin{proof}
	'$\Rightarrow$': First Assume that $f$ is continuous at $\bar{x}$. Then, given $\varepsilon > 0$, there exists $\delta > 0$ such that
	\[
		\forall x \in D, \quad |x - \bar{x}| < \delta \quad \Rightarrow \quad |f(x) - f(\bar{x})| < \varepsilon.
	\]
	Also, since $x_n \to \bar{x}$, there exists $N \in \mathbb{N}$ such that
	\[
		n \geq N \quad \Rightarrow \quad |x_n - \bar{x}| < \delta.
	\]
	Thus,
	\[
		n \geq N \quad \Rightarrow \quad |f(x_n) - f(\bar{x})| < \varepsilon,
	\]
	which implies that the sequence $(f(x_n))_{n=0}^{\infty}$ converges to $f(\bar{x})$.
	
	'$\Leftarrow$': To prove the converse, assume that $f$ is not continuous at $\bar{x}$. This means that there exists $\varepsilon > 0$ such that, for every $\delta > 0$, there is $x \in D$ with
	\[
		|x - \bar{x}| < \delta \quad \text{and} \quad |f(x) - f(\bar{x})| \geq \varepsilon.
	\]
	Now, for every $n \in \mathbb{N}$, we apply this property with $\delta = 2^{-n}$ to find a point $x_n \in D$ such that
	\[
		|x_n - \bar{x}| < 2^{-n} \quad \text{and} \quad |f(x_n) - f(\bar{x})| \geq \varepsilon
	\]
	Then the sequence constructed in this way satisfies $x_n \to \bar{x}$ but $f(x_n) \not\to f(\bar{x})$. \qedhere
\end{proof}

\begin{remark}
	\label{rmk:neg_seq_cont}
	The proof above shows that if $f:D \to \mathbb{R}$ is not continuous at $\bar{x}$, then there exists $\varepsilon > 0$ and a sequence $(x_n)_{n=0}^{\infty} \subseteq D$ with $x_n \to \bar{x}$ such that $|f(x_n) - f(\bar{x})| \geq \varepsilon$ for all $n \in \mathbb{N}$. This is useful to show that a function $f$ is not continuous at $\bar{x}$.   
\end{remark}

\subsection{Continuous Functions}

\subsubsection{Intermediate Value Theorem}
In this section we prove a fundamental theorem that formalizes the idea that the graph of a continuous function on an interval is a continuous curve, and thus cannot make any jumps.

\begin{theorem}{Intermediate Value Theorem}{int_val_theo}
	Let $f:[a, b] \to \mathbb{R}$ be a continuous function with $f(a) \leq f(b)$. Then, for every real number $c$ with $f(a) \leq c \leq f(b)$, there exists $\bar{x} \in [a, b]$ such that $f(\bar{x}) = c$.
\end{theorem}

\begin{proof}
	Fix $c \in [f(a), f(b)]$. Then define
	\[
		X = \{x \in [a, b] \;|\; f(x) \leq c\}.
	\]
	Since $a \in X$ and $X \subseteq [a, b]$, the set is non-empty and bounded from above. By Theorem \ref{theo*sup_exist}, its supremum
	\[
		\bar{x} = sup(X) \in [a, b]
	\]
	exists.
	We now use the continuity of $f$ at $x_0$ to show that $f(\bar{x}) = c$.
	
	Since $\bar{x}$ is the supremum of $X$, for each $n \geq 0$, we can find a point $x_n \in [\bar{x} - 2^{-n}, \bar{x}]$.
	Then $|x_n - \bar{x}| \leq 2^{-n}$, hence $x_n \longrightarrow \bar{x}$.
	Also, by the definition of $X$, we have $f(x_n) \leq c$. Thus, by Theorem \ref{theo*cont_seq_cont} (continuity of $f$ along sequences),
	\[
		\lim_{n \to \infty} f(x_n) = f(\bar{x}).
	\]
	And Proposition \ref{prop*lim_ineq} yields $\lim_{n \to \infty} f(x_n) \leq c$. Therefore, $f(\bar{x}) \leq c$.
	
	Suppose, by contradiction, $f(\bar{x}) < c$ and set $\varepsilon := c - f(\bar{x}) > 0$. By continuity at $\bar{x}$, there exists $\delta > 0$ such that for all $x \in [a, b]$
	\[
		|x - \bar{x}| < \delta \quad \Rightarrow \quad |f(x) - f(\bar{x})| < \varepsilon,
	\]
	hence $f(x) < f(\bar{x}) + \varepsilon = c$. 
	Therefore, by the definition of $X$,
	\[
		(\bar{x} - \delta, \bar{x} + \delta) \cap [a, b] \subseteq X.
	\]
	Moreover, since $f(\bar{x}) < c \leq f(b)$, we cannot have $\bar{x} = b$; hence $\bar{x} < b$.
	Because $\bar{x} < b$, the interval $(\bar{x} , \bar{x} + \delta) \cap [a, b] \subseteq X$ is non-empty.
	Pick 
	\[
		y \in (\bar{x}, \bar{x} + \delta) \cap [a, b] \subseteq X.
	\]
	Then $y \in X$ and $y > \bar{x}$, which contradicts the defining property of the supremum: $\bar{x}$ is an upper bound of $X$, and $X$ cannot contain elements larger than $\bar{x}$. This contradiction shows that $f(\bar{x}) \geq c$. Together with $f(\bar{x}) \leq c$ proved above, we conclude that $f(\bar{x}) = c$, as desired. \qedhere
\end{proof}

\begin{theorem}{Inverse Function Theorem}{inv_func_theo}
	Let $I$ be an interval and $f: I \to \mathbb{R}$ a continuous strictly monotone function. Then $f(I)$ is an interval, and the mapping $f:I \to f(I)$ has a continuous strictly monotone inverse function $f^{-1}:f(I) \to I$.
\end{theorem}

\begin{proof}
	We may assume that $I$ is non-empty and not a single point.
	Also, w.l.o.g, suppose $f$ is strictly increasing (otherwise replace $f$ with $-f$).
	
	Let $J = f(I)$. Since $f$ is strictly monotone it is injective. Also, since by definition $J = f(I)$, it is surjective, hence bijective. 
	Therefore there exists a unique inverse $g = f^{-1}:J \to I$.
	
	Because $f$ is strictly increasing, we have
	\begin{equation}
		\label{eq:f_strict_incr}
		x_1 < x_2 \quad \Leftrightarrow \quad f(x_1) < f(x_2) \qquad \forall x_1, x_2 \in I.
	\end{equation}
	(Note: here we have equivalence in the statements because $f$ is both injective and strictly increasing)
	Defining $y_1 = f(x_1)$ and $y_2 = f(x_2)$, this is equivalent to
	\[
		y_1 < y_2 \quad \Leftrightarrow \quad g(y_1) < g(y_2) \qquad \forall y_1, y_2 \in J
	\]
	Thus, $g$ is strictly increasing.
	
	To show that $J$ is an interval, $y_1, y_2 \in J$, and assume w.l.o.g that $y_ 1 < y_2$. Since, $J = f(I)$, Equation \eqref{eq:f_strict_incr} implies that $y_1 = f(x_1), y_2 = f(x_2)$ for some $x_1, x_2 \in I$ with $x_1 < x_2$.
	Now by the Intermediate Value Theorem \ref{theo*int_val_theo} applied to $f:[x_1, x_2] \to \mathbb{R}$, we have that all values $c \in [y_1, y_2]$ are in the image of $f:[x_1, x_2] \to \mathbb{R}$, i.e.,
	\[
		[y_1, y_2] \subseteq f([x_1, x_2]) \subseteq J.
	\]
	Since, $y_1, y_2$ were two arbitrary points in $J$, this proves that $J$ is an interval.
	
	It remains to show that $g = f^{-1}$ is continuous. 
	Fix $\bar{y} \in J$ and suppose, by contradiction, that $g$ is not continuous at $\bar{y}$. Then by Remark \ref{rmk:neg_seq_cont}, there exists $\varepsilon > 0$ and a sequence $(y_n)_{n=0}^{\infty} \subseteq J$ such that
	\begin{equation}
		\label{eq:g_not_cont}
		y_n \longrightarrow \bar{y} \qquad \text{but} \qquad |g(y_n) - g(\bar{y})| \geq \varepsilon \quad \forall n \in \mathbb{N}.
	\end{equation}
	Set $x_n = g(y_n) \in I$ and $\bar{x} = g(\bar{y}) \in I$.
	Then for every $n \in \mathbb{N}$, either $x_n \leq \bar{x} - \varepsilon$ or $x_n \geq \bar{x} + \varepsilon$.
	In particular, at least one of these cases must occur infinitely often. W.l.o.g, assume $x_n \leq \bar{x} - \varepsilon$ for infinitely many $n$, and extract a subsequence $(x_{n_k})_{k=0}^{\infty}$ with $x_{n_k} \leq \bar{x} - \varepsilon$ for all $k$. 
	Since, $I$ is an interval, $\bar{x} - \varepsilon \in I$, and by strict monotonicity of $f$ we obtain
	\[
		y_{n_k} = f(x_{n_k}) \leq f(\bar{x} - \varepsilon) < f(\bar{x}) = \bar{y}.	
	\]
	Then Proposition \ref{prop*lim_ineq} gives (recall $y_n \longrightarrow \bar{y}$, see \eqref{eq:g_not_cont})
	\[
		\bar{y} = \lim_{k \to \infty} y_{n_k} \leq f(\bar{x} - \varepsilon) < f(\bar{x}) = \bar{y},
	\]
	a contradiction. Hence, $g$ is continuous. \qedhere
\end{proof}

\subsection{Continuous Functions on Compact Intervals}
In this section we show that continuous functions on \textbf{bounded closed} intervals, called \textbf{compact intervals}, enjoy special properties.

\subsubsection{Boundedness and Extrema}

\begin{lemma}{Compactness}{compact}
	Let $[a, b]$ be a compact interval, and let $(x_n)_{n=0}^{\infty}$ be a sequence contained in $[a, b]$.
	Then there exists a subsequence $(x_{n_k})_{k=0}^{\infty}$ such that
	\[
		\lim_{k \to \infty} x_{n_k} = \bar{x} \qquad \text{for some } \bar{x} \in [a, b].
	\]
\end{lemma}

\begin{proof}
	Since $(x_n)_{n=0}^{\infty}$ is bounded (as it lies in $[a, b]$), Corollary \ref{cor*bound_seq_conv_subseq} ensures the existence of a convergent subsequence $(x_{n_k})_{k=0}^{\infty}$. Let $\bar{x}$ denote its limit. Because $a \leq x_{n_k} \leq b$ for all k, Proposition \ref{prop*lim_ineq} yields $a \leq \bar{x} \leq b$. \qedhere
\end{proof}

\begin{theorem}{Boundedness}{boundedness}
	Let $[a, b]$ be compact interval, and let $f:[a, b] \to \mathbb{R}$ be continuous. Then $f$ is bounded.
\end{theorem}

\begin{proof}
	Assume by contradiction that $f$ is unbounded. Then, for every $n \in \mathbb{N}$, there exists $x_n \in [a, b]$ such that $|f(x_n)| \geq n$. By Lemma \ref{lem*compact}, there is a subsequence $(x_{n_k})_{k=0}^{\infty}$ converging to some $\bar{x} \in [a, b]$.
	
	Since $f$ is continuous, so is $|f|$ (recall Remark \ref{rmk:modulus_cont}), therefore $|f(x_{n_k})| \longrightarrow |f(\bar{x})| \in \mathbb{R}$. This contradicts $|f(x_{n_k})| \geq n_k \longrightarrow \infty$, so $f$ must be bounded. \qedhere
\end{proof}

\begin{exercise}
	Find examples of:
	\begin{enumerate}
		\item a continuous but unbounded function on a bounded \textit{open} interval.
		\[
			f:(0, 1) \to \mathbb{R},\; x \mapsto \frac{1}{x}.
		\]
		\item a continuous but unbounded function on an \textit{unbounded closed} interval.
		\[
			f:[0, \infty) \to \mathbb{R},\; x \mapsto x.
		\]
		\item an unbounded function on a compact interval but discontinuous at only one point.
		\[
			f:[0, 1] \to \mathbb{R},\; x \mapsto \begin{cases}
				\frac{1}{x}, \quad &\text{for } x \neq 0\\
				a \in \mathbb{R}, \quad &\text{for } x = 0.
			\end{cases}
		\]
	\end{enumerate}
\end{exercise}

\begin{definition}{Extreme Values}{extreme_val}
	Let $D \subseteq \mathbb{R}$ and $f: D \to \mathbb{R}$.
	\begin{itemize}
		\item[$\bullet$] We say that $f$ takes its \textbf{maximum value} at $x_0 \in D$ if $f(x) \leq f(x_0)$ for all $x \in D$.
		Then $f(x_0)$ is the \textbf{maximum} of $f$.
		\item[$\bullet$] We say that $f$ takes its \textbf{minimum value} at $x_0 \in D$ if $f(x) \geq f(x_0)$ for all $x \in D$.
		Then $f(x_0)$ is the \textbf{minimum} of $f$.
	\end{itemize}
	Maxima and minima ar called \textbf{extreme values} or \textbf{extrema}.
\end{definition}

\begin{theorem}{Extreme Value Theorem}{extreme_val_theo}
	Let $[a, b]$ be a compact interval, and let $f:[a, b] \to \mathbb{R}$ be continuous. Then $f$ attains both its minimum and its maximum.
\end{theorem}

\begin{proof}
	Theorem \ref{theo*boundedness} guarantees that $f$ is bounded, or equivalently, that $f([a, b]) \subseteq \mathbb{R}$ is a bounded subset of $\mathbb{R}$. Thus, Theorem \ref{theo*sup_exist} implies that
	\[
		S := \sup f([a, b])
	\]
	exists.
	By definition of the supremum, for each $n \in \mathbb{N}$ there exists $y_n \in f([a, b])$ such that $S - 2^{-n} \leq y_n \leq S$. Hence, $y_n \to S$. Also, since $y_n \in f([a, b])$, there exists $x_n \in [a, b]$ such that $f(x_n) = y_n$.
	
	Now, by Lemma \ref{lem*compact}, we can find a subsequence $(x_{n_k})_{k=0}^{\infty}$ such that $x_{n_k} \to \bar{x} \in [a, b]$. By continuity of $f$, we have that
	\[
		f(\bar{x}) = \lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} y_{n_k} = S,
	\]
	so $f$ attains its maximum at $\bar{x}$.
	
	Applying the same reasoning to $-f$ shows that $f$ also attains its minimum. \qedhere
\end{proof}

\subsubsection{Uniform Continuity}

\begin{definition}{Uniform Continuity}{unif_cont}
	Let $D \subseteq \mathbb{R}$. A function $f:D \to \mathbb{R}$ is \textbf{uniformly continuous} if, for every $\varepsilon > 0$, there exists $\delta > 0$ such that
	\[
		|x - y| < \delta \quad \Rightarrow \quad |f(x) - f(y)| < \varepsilon \qquad \forall x,y \in D.
	\]
\end{definition}

\begin{remark}
	The difference between the usual definition of continuity and the one of uniform continuity lies in how the choice of $\delta$ depends on the points considered.
	
	For a function that is continuous at each $x_0 \in D$, the $\delta$ in the definition may depend on both $\varepsilon$ \textbf{and} $x_0$: for every $\varepsilon > 0$ and each $x_0$, we can find a $\delta = \delta(\varepsilon, x_0)$ that works near $x_0$.
	
	Uniform continuity is stronger: there exists a single $\delta = \delta(\varepsilon)$ that works \textbf{simultaneously} for all $x, y \in D$. In other words, the control on the variation of $f$ does not deteriorate as we move along the domain. This property is automatically satisfied on compact intervals for continuous functions, as we will prove below.
\end{remark}

\begin{theorem}{Uniform Continuity on Compact Intervals}{unif_cont_compact_int}
	Let $[a, b]$ be a compact interval, and $f:[a, b] \to \mathbb{R}$ continuous on $[a, b]$. Then $f$ is uniformly continuous.
\end{theorem}

\begin{proof}
	Assume, by contradiction, that $f$ is not uniformly continuous on $[a, b]$. Then there exists $\varepsilon > 0$ such that for every $\delta > 0$ one can find $x, y \in [a, b]$ with
	\[
		|x - y| < \delta \quad \text{and} \quad |f(x) - f(y)| \geq \varepsilon.
	\]
	Taking $\delta = 2^{-n}$ for each $n \in \mathbb{N}$, we obtain sequences $(x_n)_{n=0}^{\infty}$ and $(y_n)_{n=0}^{\infty}$ in $[a, b]$ with
	\begin{equation}
		\label{eq:contra_unif_cont}
		|x_n - y_n| < 2^{-n} \quad \text{and} \quad |f(x_n) - f(y_n)| \geq \varepsilon.
	\end{equation}
	By Lemma \ref{lem*compact}, the sequence $(x_n)_{n=0}^{\infty}$ has a subsequence $(x_{n_k})_{k=0}^{\infty}$ converging to some $\bar{x} \in [a, b]$. Then
	\[
		|y_{n_k} - \bar{x}| \leq |y_{n_k} - x_{n_k}| + |x_{n_k} - \bar{x}| < 2^{-n_k} + |x_{n_k} - \bar{x}| \underset{k \to \infty}{\longrightarrow} 0,
	\]
	so $y_{n_k} \to \bar{x}$ as well. Thus, by continuity of $f$ and Theorem \ref{theo*cont_seq_cont}, we have that
	\[
		\lim_{k \to \infty} f(x_{n_k}) = \lim_{k \to \infty} f(y_{n_k}) = f(\bar{x}),
	\]
	therefore,
	\[
		|f(x_{n_k}) - f(y_{n_k})| \leq |f(x_{n_k}) - f(\bar{x})| + |f(\bar{x}) - f(y_{n_k})| \underset{k \to \infty}{\longrightarrow} 0,
	\]
	which contradicts Equation \eqref{eq:contra_unif_cont}. Hence, $f$ is uniformly continuous on $[a, b]$. \qedhere
\end{proof}

\begin{definition}{Lipschitz Continuity}{lip_cont}
	Let $D \subseteq \mathbb{R}$, and $f: D \to \mathbb{R}$. We say that $f$ is \textbf{Lipschitz continuous} if there exists $L \geq 0$ such that
	\[
		|f(x) - f(y)| \leq L|x - y| \qquad \forall x, y \in D.
	\]
\end{definition}

\begin{lemma}{Lipschitz Continuity $\Rightarrow$ Uniform Continuity}{lip_cont_unif_cont}
	Let $D \subseteq \mathbb{R}$, and $f:D \to \mathbb{R}$ be a Lipschitz continuous function. Then $f$ is uniformly continuous.
\end{lemma}

\begin{proof}
	Let $D \subseteq \mathbb{R}$ and assume that $f:D \to \mathbb{R}$ is a Lipschitz continuous function. Then there exists $L \geq 0$ such that
	\[
		|f(x) - f(y)| \leq L|x - y| \qquad \forall x, y \in D.
	\]
	Now, fix $\varepsilon > 0$. We assume that $L \neq 0$ (otherwise the result follows immediately) and choose $\delta = \frac{\varepsilon}{L}$.
	Because of the Lipschitz continuity of $f$, we have that for all $x, y \in D$ it holds that
	\begin{align*}
		|x - y| < \delta = \frac{\varepsilon}{L} \quad \Leftrightarrow \quad L|x - y| < \varepsilon\\
		\Rightarrow \qquad |f(x) - f(y)| \leq L|x - y| < \varepsilon,
	\end{align*}
	which shows that $f$ is also uniformly continuous.\qedhere
\end{proof}


\subsection{Example: Exponential and Logarithmic Functions}

\subsubsection{Definition of the Exponential Function}

\begin{lemma}{Bernoulli's Inequality}{bernoulli_ineq}
	For all $a \in \mathbb{R}$ with $a \geq -1$ and all $n \in \mathbb{N}$ with $n \geq 1$, it holds that
	\[
		(1 + a)^{n} \geq 1 + na.
	\]
\end{lemma}

\begin{proof}
	We proceed by induction. For $n = 1$ we have $(1 + a)^1 = 1 + a = 1 + 1 \cdot a$.
	
	Now assume that the inequality holds for some $n \geq 1$. Since $1 + a \geq 0$ by assumption, we find
	\[
		(1 + a)^{n + 1} = (1 + a)^n (1 + a) \geq (1 + n a) (1 + a) = 1 + na + a + na^2 \geq 1 + (n + 1)a,
	\]
	which establishes the induction step and completes the proof. \qedhere
\end{proof}

\begin{proposition}{Existence of the Exponential}{exp_exist}
	Let $x \in \mathbb{R}$. The sequence $(a_n)_{n=1}^{\infty}$ defined by
	\[
		a_n = \left(1 + \frac{x}{n}\right)^n
	\]
	is convergent, and its limit is a positive real number.
\end{proposition}

\begin{lemma}{Monotonicity}{exp_mono}
	Given $x \in \mathbb{R}$, let $n_0 \in \mathbb{N}$ satisfy $n_0 \geq 1$ and $n_0 > -x$. Then the sequence $(a_n)_{n=n_0}^{\infty}$ defined in Proposition \ref{prop*exp_exist} is increasing.
\end{lemma}

\begin{definition}{Exponential Function}{exp}
	The \textbf{exponential function} $\exp:\mathbb{R} \to \mathbb{R}_{>0}$ is defined by
	\[
		\exp(x) = \lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n \qquad \forall x \in \mathbb{R}.
	\]
\end{definition}

\begin{corollary}{Growth of the Exponential}{growth_exp}
	Given $n \in \mathbb{N}$ with $n \geq 1$, the exponential function satisfies
	\[
		\exp(x) \geq \left(1 + \frac{x}{n}\right)^n \qquad \forall x > -n.
	\]
\end{corollary}

\begin{proof}
	By Lemma \ref{lem*exp_mono} and Definition \ref{def*exp}, for $x > -n$ we have
	\[
		a_n \leq a_{n+1} \leq \hdots \leq \exp(x). \qedhere
	\]
\end{proof}

\subsubsection{Properties of the Exponential Function}

\begin{theorem}{Properties of the Exponential Function}{exp_properties}
	The exponential function $\exp:\mathbb{R} \to \mathbb{R}_{>0}$ is bijective, strictly increasing, and continuous. Moreover,
	\begin{align*}
		\exp(0) &= 1,\\
		\exp(-x) &= \exp(x)^{-1},\\
		\exp(x + y) &= \exp(x)\exp(y),
	\end{align*}
	for all $x, y \in \mathbb{R}$.
\end{theorem}

\subsubsection{The Natural Logarithm}
\begin{definition}{Logarithm}{log}
	The unique inverse function
	\[
		\log:\mathbb{R}_{>0} \to \mathbb{R}
	\]
	of the bijective map $exp:\mathbb{R} \to \mathbb{R}_{>0}$ is called the \textbf{logarithm}.
\end{definition}

\begin{corollary}{Properties of the Logarithm}{log_properties}
	The logarithm $\log:\mathbb{R}_{>0} \to \mathbb{R}$ is strictly increasing, continuous, and bijective. Moreover,
	\begin{align*}
		\log(1) &= 0,\\
		\log(a^{-1}) &= -\log(a),\\
		\log(ab) &= \log(a) + \log(b),
	\end{align*}
	for all $a, b \in \mathbb{R}_{>0}$.
\end{corollary}

The logarithm defined here is also called the \textbf{natural logarithm} to distinguish it from logarithms with another \textbf{base} $a > 1$ (for instance $a = 10$ or $a = 2$). For any $a > 1$, we define
\[
	\log_a(x) = \frac{\log(x)}{\log(a)} \qquad \forall x > 0.
\]
Unless stated otherwise, $\log(x)$ always denotes the natural logarithm, i.e., the logarithm to base $e$.

We can now define powers with arbitrary real exponents. For $a > 0$ and $x \in \mathbb{R}$ we set
\[
	a^x = \exp(x\,\log(a)).
\]

\subsection{Limits of Functions}
We consider functions $f:D \to \mathbb{R}$ defined on a subset $D \subseteq \mathbb{R}$, and we wish to define the limit of $f(x)$ as $x \in D$ approaches a point $x_0 \in \mathbb{R}$. Typical examples include $D = \mathbb{R}, D = [0, 1]$ or $D = (0, 1)$, with $x_0 = 0$ in each case.

\subsubsection{Limit in the Vicinity of a Point}
\label{sec:lim_vicinity_pt}
Let $D \subseteq \mathbb{R}$ be non-empty, and let $x_0 \in \mathbb{R}$ be such that
\begin{equation}
	\label{eq:acc_pt}
	D \cap (x_0 - \delta, x_0  + \delta) \neq \emptyset
\end{equation}
for all $\delta > 0$. Whenever this holds, we say that $x_0$ is an \textbf{accumulation point} of $D$. Note that if $x_0 \in D$, then Equation \eqref{eq:acc_pt} is automatically satisfied.

Condition \ref{eq:acc_pt} ensures that there exists a sequence of points in $D$ converging to $x_0$.

\begin{definition}{Limit of a Function}{lim_func}
	Let $f:D \to \mathbb{R}$, and $x_0$ be an accumulation point of $D$.
	A number $L \in \mathbb{R}$ is called the \textbf{limit of} $f(x)$ \textbf{as} $x \to x_0$ if, for every $\varepsilon > 0$, there exists $\delta > 0$ such that
	\[
		|x - x_0| < \delta \quad \Rightarrow \quad |f(x) - L| < \varepsilon \qquad \forall x \in D.
	\]
\end{definition}
In general, the limit of $f(x)$ as $x \to x_0$ may not exist. However, if it exists, it is uniquely determined. Hence we speak of \textit{the} limit and write
\[
	\lim_{x \to x_0} f(x) = L
\]
to indicate the limit exists and is equal to $L$. Informally, this means that the function values $f(x)$ are arbitrarily close to $L$ whenever $x \in D$ is sufficiently close to $x_0$.

The limit of a function satisfies properties analogous to those of Proposition \ref{prop*lim_ineq}. More precisely, if $f, g$ are functions on $D$ such that
\[
	\lim_{x \to x_0} f(x) = L_1 \qquad \text{and} \qquad \lim_{x \to x_0} g(x) = L_2,
\]
then
\[
	\lim_{x \to x_0} (f+g)(x) = L_1 + L_2, \qquad \lim_{x \to x_0} (f\cdot g)(x) = L_1\cdot L_2.
\]
Moreover, $f \leq g$ implies $L_1 \leq L_2$, and the sandwich lemma holds: if $f \leq h \leq g$ and $L_1 = L_2$ then $\lim_{x \to x_0} h(x) = L_1 = L_2$.

\begin{remark}
	Let $f:D \to \mathbb{R}$ be a function. If $x_0 \in D$, then $f$ is continuous at $x_0$ if and only if $\lim_{x \to x_0} f(x) = f(x_0)$.
\end{remark}

Suppose that $x_0 \in D$ is an accumulation point of $D \setminus \{x_0\}$. Let $f:D \to \mathbb{R}$, and consider the restriction $f|_{D \setminus \{x_0\}}$. It may happen that $f$ is discontinuous at $x_0$, but the limit
\begin{equation}
	L = \lim_{x \to x_0} f|_{D \setminus \{x_0\}}(x)
\end{equation}
nevertheless exists. In this case, the point $x_0$ is called a \textbf{removable discontinuity} of $f$, and one also writes
\begin{equation}
	\label{eq:limit_neq_x}
	L = \underset{x \neq x_0}{\lim_{x \to x_0}} f(x).
\end{equation}
If we now define
\begin{equation}
	\label{eq:cont_ext}
	\tilde{f}(x) = \begin{cases}
		f(x), \quad &x \in D \setminus \{x_0\},\\
		L, \quad &x = x_0,
	\end{cases}
\end{equation}
then $\tilde{f}$ is continuous at $x_0$. In other words, we can remove the discontinuity of $f$ by redefining its value at $x_0$ to be $L$.

If instead $x_0 \notin D$ but the limit in Equation \eqref{eq:limit_neq_x} exists, we call the function $\tilde{f}$ defined in Equation \eqref{eq:cont_ext} the \textbf{continuous extension} of $f$ to $D \cup \{x_0\}$.

Arguing as in the proof of Theorem \ref{theo*cont_seq_cont}, we obtain the following result.

\begin{lemma}{Limit and Sequences}{lim_and_seq}
	Let $f:D \to \mathbb{R}$. Then $L = \lim_{x \to \bar{x}} f(x)$ if and only if, for every sequence $(x_n)_{n=0}^{\infty} \subseteq D$ converging to $\bar{x}$, one has $\lim_{n \to \infty} f(x_n) = L$.
\end{lemma}

We now state a result describing the behaviour of limits under composition with a continuous function.

\begin{proposition}{Limit and Composition}{lim_and_comp}
	Let $E \subseteq \mathbb{R}$, and let $f:D \to E$ be such that the limit $L = \lim_{x \to \bar{x}} f(x)$ exists and belongs to $E$. If $g:E \to \mathbb{R}$ is continuous at $L$, then
	\[
		\lim_{x \to \bar{x}} g(f(x)) = g(L).
	\]
\end{proposition}

\begin{proof}
	Let $(x_n)_{n=0}^{\infty} \subseteq D$ be a sequence converging to $\bar{x}$. By Lemma \ref{lem*lim_and_seq}, we have $\lim_{n \to \infty} f(x_n) = L$. Since $g$ is continuous at $L$, Theorem \ref{theo*cont_seq_cont} gives $\lim_{n \to \infty} g(f(x_n)) = g(L)$. Because $(x_n)_{n=0}^{\infty}$ was arbitrary, using Lemma \ref{lem*lim_and_seq} again, we conclude that $\lim_{x \to \bar{x}} g(f(x)) = g(L)$. \qedhere
\end{proof}

We now introduce conventions for improper limits of functions, in analogy with improper limits for sequences.

\begin{definition}{Improper Limits}{improper_lim}
	Let $f:D \to \mathbb{R}$, and let $x_0$ be an accumulation point of $D$.
	We say that $f$ \textbf{diverges to} $+\infty$ \textbf{as} $x \to x_0$, and write
	\[
		\lim_{x \to x_0} f(x) = + \infty,
	\]
	if for every $M > 0$, there exists $\delta > 0$ such that
	\[
		\forall x \in D: \quad |x - x_0| < \delta \quad \Rightarrow \quad f(x) \geq M.
	\]
	Analogously, $f$ \textbf{diverges to} $-\infty$ \textbf{as} $x \to x_0$ and we write $\lim_{x \to x_0} f(x) = -\infty$, if for every $M > 0$, there exists $\delta > 0$ such that
	\[
		\forall x \in D: \quad |x - x_0| < \delta \quad \Rightarrow \quad f(x) \leq -M.
	\]
\end{definition}

\subsubsection{One-Sided Limits}
It is often useful to consider limits taken form one side only and to allow $x_0$ to be $\pm \infty$ as well. To this end, let $x_0 \in \mathbb{R}$ be such that
\begin{equation}
	\label{eq:x_right_acc_pt}
	D \cap (x_0, x_0 + \delta) \neq \emptyset
\end{equation}
for every $\delta > 0$. In this case, we say that $x_0$ is a \textbf{right-hand accumulation point} of $D$. Analogously, if
\begin{equation}
	\label{eq:x_left_acc_pt}
	D \cap (x_0 - \delta, x_0) \neq \emptyset
\end{equation}
for every $\delta > 0$, we say that $x_0$ is a \textbf{left-hand accumulation point} of $D$.

\begin{definition}{One-Sided Limits}{one_side_lim}
	Let $f:D \to \mathbb{R}$, and let $x_0 \in \mathbb{R}$ be a right-hand accumulation point of $D$. A number $L \in \mathbb{R}$ is called the \textbf{right-hand limit} of $f$ at $x_0$ if, for every $\varepsilon > 0$, there exists $\delta > 0$ such that
	\[
		x \in D \cap (x_0, x_0 + \delta) \quad \Rightarrow \quad |f(x) - L| < \varepsilon.
	\]
	In this case we write $L = \lim_{x \to x_0^+} f(x)$.
	We also allow improper one-sided limits. We say that
	\[
		\lim_{x \to x_0^+} f(x) = +\infty
	\]
	if for every $M > 0$, there exists $\delta > 0$ such that
	\[
		x \in D \cap (x_0, x_0 + \delta) \quad \Rightarrow \quad f(x) \geq M.
	\]
	Similarly, $\lim_{x \to x_0^+} f(x) = -\infty$ means that, for every $M > 0$, there exists $\delta > 0$ such that
	\[
		x \in D \cap (x_0, x_0 + \delta) \quad \Rightarrow \quad f(x) \leq -M.
	\]
	The \textbf{left-hand limit} is defined analogously, considering a left-hand accumulation point of $D$ and writing $\lim_{x \to x_0^-} f(x)$.
\end{definition}

Next, we define the notion of limit at infinity.

\begin{definition}{Limits at Infinity}{lim_at_inf}
	Let $f:D \to \mathbb{R}$, and assume that $D \cap (R, \infty) \neq \emptyset$ for every $R > 0$.
	A number $L \in \mathbb{R}$ is called the \textbf{limit of} $f$ \textbf{as} $x \to +\infty$ if, for every $\varepsilon > 0$, there exists $R > 0$ such that
	\[
		x \in D \cap (R, \infty) \quad \Rightarrow \quad |f(x) - L| < \varepsilon.
	\]
	We say that $f$ \textbf{diverges to} $+\infty$ \textbf{as} $x \to +\infty$ if, for every $M > 0$, there exists $R > 0$ such that
	\[
		x \in D \cap (R, \infty) \quad \Rightarrow \quad f(x) \geq M.
	\]
	The corresponding definition for $x \to -\infty$ and diverges to $-\infty$ are analogous.
\end{definition}

Limits at $+\infty$ can be converted into right-hand limits at 0 via inversion. Given $f:D \to \mathbb{R}$ as above, define
\[
	E = \{x > 0 \;|\; x^{-1} \in D\}, \qquad g:E \to \mathbb{R}, \qquad g(x) = f(x^{-1}).
\]
Then 
\[
	\lim_{x \to +\infty} f(x) = \lim_{x \to 0^+} g(x),  
\]
so one limit exists if and only if the other does.

\begin{definition}{One-Sided Continuity and Jumps}{one_side_cont_jump}
	Let $f:D \to \mathbb{R}$ and $x_0 \in D$. If $\lim_{x \to x_0^+} f(x)$ exists and equals $f(x_0)$, then $f$ is \textbf{continuous from the right} at $x_0$. \textbf{Continuity form the left} is defined similarly.
	We call $x_0$ a \textbf{jump point} if both one-sided limits exist but are different, i.e.,
	\[
		L_{-} := \lim_{x \to x_0^-} f(x) \in \mathbb{R}, \qquad L_+ := \lim_{x \to x_0^+} f(x) \in \mathbb{R}, \qquad L_{-} \neq L_+.
	\]
\end{definition}

\subsubsection{Landau Notation}
We introduce two standard notations that compare the asymptotic behaviour of a function to that of another function. (often called \textit{relative} asymptotics).

\begin{definition}{Big-O at a Point}{big_o}
	Let $f,g:D \to \mathbb{R}$, and let $x_0$ be an accumulation point of $D$. We write
	\[
		f(x) = O(g(x)) \quad \text{as } x \to x_0
	\]
	if there exists $M > 0$ and $\delta > 0$ such that
	\[
		x \in D \cap (x_0 - \delta, x_0 + \delta) \quad \Rightarrow \quad |f(x)| \leq M |g(x)|.
	\]
	We then say that $f$ is a \textbf{Big-O} of $g$ as $x \to x_0$.
\end{definition}
If $g(x) \neq 0$ for all $x$ sufficiently close to $x_0$ (with $x \in D$), then
\[
	f(x) = O(g(x)) \quad \text{as } x \to x_0 \qquad \Leftrightarrow \qquad \frac{f(x)}{g(x)} \text{ is bounded near $x_0$}.
\]

\begin{definition}{Big-O at Infinity}{big_o_inf}
	Let $f, g:D \to \mathbb{R}$, and assume $D \cap (R, \infty) \neq \emptyset$ for every $R > 0$. We write
	\[
		f(x) = O(g(x)) \quad \text{as } x \to +\infty
	\]
	if there exists $M > 0$ and $R > 0$ such that
	\[
		x \in D \cap (R, \infty) \quad \Rightarrow \quad |f(x)| \leq M |g(x)|.
	\]
	The definition for $x \to -\infty$ is analogous.
\end{definition}

The big-O notation hides the precise bound by an \textit{implicit constant} $M$, which is often irrelevant for the argument one is interested in.

\subsubsection*{Example}
\begin{itemize}
	\item[$\bullet$] if $f$ and $g$ are bounded and continuous near $x_0$ with $g(x_0) \neq 0$, then $f(x) = O(g(x))$ as $x \to x_0$.
	\item[$\bullet$] As $x \to 0$, one has $x^2 = O(x)$, but $x \neq O(x^2)$ (since $x / x^2$ is unbounded near 0).
	\item[$\bullet$] As $x \to +\infty$, $\frac{3x^3}{x^3 + 3} = O(1)$, but $\frac{3x^3}{x^3 + 3} \neq O(x^{\alpha})$ for $\alpha < 0$.
\end{itemize}

As discussed above, the big-O means that $f$ is bounded by a multiple of $g$. One may also consider a stronger condition, namely that $f$ is asymptotically negligible with respect to $g$. This leads to the following definition.

\begin{definition}{Little-O at a Point}{lil_o}
	Let $f, g:D \to \mathbb{R}$, and let $x_0$ be an accumulation point of $D$. We write
	\[
		f(x) = o(g(x)) \quad \text{as } x\to x_0
	\]
	if, for every $\varepsilon > 0$ there exists $\delta > 0$ such that
	\[
		x \in D \cap (x_0 - \delta, x_0 + \delta) \quad \Rightarrow \quad |f(x)| \leq \varepsilon |g(x)|.
	\]
	We then say that $f$ is a \textbf{little-o} of $g$ as $x \to x_0$.
\end{definition}

If $g(x) \neq 0$ for all $x$ near $x_0$ (with $x \in D$), then
\[
	f(x) = o(g(x)) \quad \text{as } x \to x_0 \qquad \Leftrightarrow \qquad \lim_{x \to x_0} \frac{f(x)}{g(x)} = 0.
\]
Moreover, $f(x) = o(g(x)) \; \Rightarrow \; f(x) = O(g(x))$.

\begin{definition}{Little-o at Infinity}{lil_o_inf}
	Let $f, g:D \to \mathbb{R}$, and assume that $D \cap (R, \infty) \neq \emptyset$ for every $R > 0$. We write
	\[
		f(x) = o(g(x)) \quad \text{as } x \to +\infty
	\]
	if, for every $\varepsilon > 0$ there exists $R > 0$ such that
	\[
		x \in D \cap (R, \infty) \quad \Rightarrow \quad |f(x)| \leq \varepsilon |g(x)|.
	\]
	The definition for $x \to -\infty$ is analogous.
\end{definition}

\subsubsection*{Example}
\begin{itemize}
	\item[$\bullet$] $x = o(x^2)$ as $x \to +\infty$, and $x^2 = o(x)$ as $x \to 0$
	\item[$\bullet$] For any $\alpha < 1$,
	\[
		\frac{3x^3}{2x^2 + x^10} = o(|x|^{\alpha}) \quad \text{as } x\to 0,
	\]
	but not for $\alpha \geq 1$. Indeed,
	\[
		\big|\frac{3x^3}{|x|^{\alpha} 2x^2 + x^10}\big|= |x|^{1 - \alpha} \frac{3}{2 + x^8} \longrightarrow 0 \quad \text{as } x \to 0,
	\]
	whenever $\alpha < 1$.
\end{itemize}

In computations, one often uses Landau symbols as placeholders. Writing
\[
	f(x) = o(g(x)) \quad \text{as } x \to x_0
\]
means there is a function $h:D \to \mathbb{R}$ with $h(x) = o(g(x))$ as $x \to x_0$. Similarly for big-O.

\subsubsection*{Example}
Polynomial division gives, as $x \to +\infty$,
\[
	\frac{x^3 - 7x^2 + 6x + 2}{x^2} = x - 7 + O\left(\frac{1}{x}\right) = x - 7 + o(1) = x + O(1) = x + o(x).
\]

\subsection{Sequences of Functions}

\subsubsection{Pointwise Convergence}

\begin{definition}{Sequences of Functions}{seq_func}
	A \textbf{sequence} of real-valued on a subset $D \subseteq \mathbb{R}$ is a family of functions $f_n: D \to \mathbb{R}$ indexed by $\mathbb{N}$. The function $f_n$ is called the n-th \textbf{element} of the sequence. One often writes $(f_n)_{n \in \mathbb{N}}$, $(f_n)_{n=0}^{\infty}$, or $(f_n){n\geq 0}$ for a sequence of functions.
\end{definition}

\begin{definition}{Pointwise Convergence}
	Let $D \subseteq \mathbb{R}$, and let $(f_n)_{n=0}^{\infty}$ be a sequence of functions $f_n:D \to \mathbb{R}$. Let $f:D \to \mathbb{R}$ be another function. We say that $(f_n)_{n=0}^{\infty}$ \textbf{converges pointwise} to $f$, if for every $x \in D$, the sequence of real numbers $(f_n(x))_{n=0}^{\infty}$ converges to $f(x)$, i.e., for every $x \in D$ and for every $\varepsilon > 0$, there exists $n \in \mathbb{N}$ such that
	\[
		|f_n(x) - f(x)| < \varepsilon \qquad \forall n \geq N.
	\]
	In this case, $f$ is called the \textbf{pointwise limit} of the sequence $(f_n)_{n=0}^{\infty}$.
\end{definition}

\begin{remark}
	Note that, in the definition of pointwise convergence the index $N$ may depend on both $x$ and $\varepsilon$. In other words, for each point $x \in D$ we examine the convergence of $(f_n)_{n=0}^{\infty}$ to $f$ separately.
\end{remark}

\subsubsection{Uniform Convergence}

\begin{definition}{Uniform Convergence}{unif_conv}
	Let $D \subseteq \mathbb{R}$, and let $(f_n)_{n=0}^{\infty}$ be a sequence of functions $f_n:D \to \mathbb{R}$. Let $f:D \to \mathbb{R}$ be another function. We say that $(f_n)_{n=0}^{\infty}$ \textbf{converges uniformly} to $f$ on $D$ if, for every $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that
	\[
		|f_n(x) - f(x)| < \varepsilon \qquad \forall n\geq N,\; \forall x \in D.
	\]
\end{definition}

\begin{remark}
	Note that, in the definition of uniform convergence the index $N$ may only depend on $\varepsilon$, and therefore the condition has to hold for all $x \in D$ for the sequence of functions $(f_n)_{n=0}^{\infty}$ to converge uniformly to $f$ on $D$.
\end{remark}

\begin{remark}
	Let $D \subseteq \mathbb{R}$, and $(f_n)_{n=0}^{\infty}$ be a sequence of functions $f_n:D \to \mathbb{R}$ converging uniformly to $f:D\to \mathbb{R}$ on $D$. Then the sequence of functions $(f_n)_{n=0}^{\infty}$ also converges pointwise to $f$.
\end{remark}

\begin{theorem}{Continuity under Uniform Convergence}{cont_unif_conv}
	Let $D \subseteq \mathbb{R}$, and let $(f_n)_{n=0}^{\infty}$ be a sequence of continuous functions converging uniformly to $f:D \to \mathbb{R}$. Then $f$ is continuous.
\end{theorem}

\begin{proof}
	To prove that $f$ is continuous, we fix $\bar{x} \in D$ and show that $f$ is continuous at $\bar{x}$. Given $\varepsilon > 0$, the uniform convergence of $f_N$ to $f$ provides $N \in \mathbb{N}$ such that
	\[
		|f_N(y) - f(y)| < \frac{\varepsilon}{3} \qquad \forall y \in D.
	\]
	Also, since $f_N$ is continuous at $\bar{x}$, there exists $\delta > 0$ such that
	\[
		|x - \bar{x}| < \delta \quad \Rightarrow \quad |f_N(x) - f_N(\bar{x})| < \frac{\varepsilon}{3}.
	\]
	Then, for $|x - \bar{x}| < \delta$, we have
	\[
		|f(x) - f(\bar{x})| \leq |f(x) - f_N(x)| + |f_N(x) - f_N(\bar{x})| + |f_N(\bar{x}) - f(\bar{x})| < \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon.
	\]
	Which shows that $f$ is continuous at $\bar{x}$. Since, $\bar{x}$ is arbitrary, $f$ is continuous on $D$.
\end{proof}

Intuitively, uniform convergence allows us to \textit{exchange} the order of taking limits. More precisely, assume $(f_n)_{n=0}^{\infty}$ is a sequence of continuous functions converging pointwise to $f$. Then, by the pointwise convergence and the continuity of the functions $f_n$ we have
\[
	f(\bar{x}) = \lim_{n \to \infty} f_n(\bar{x}), \qquad f_n(\bar{x}) = \lim_{x \to \bar{x}} f_n(x), \qquad f(x) = \lim_{n \to \infty} f_n(x) \quad \forall x \in D. 
\]
Hence,
\[
	f(\bar{x}) = \lim_{n \to \infty} f_n(\bar{x}) = \lim_{n \to \infty} \bigg(\lim_{x \to \bar{x}}f_n(x)\bigg), \qquad \lim_{x \to \bar{x}} f(x) = \lim_{x \to \bar{x}} \bigg(\lim_{n \to \infty} f_n(x)\bigg).
\]
Note that the function $f$ is continuous at $\bar{x}$ if and only if $f(\bar{x}) = \lim_{x \to \bar{x}} f(x)$, which by the identities above is equivalent to
\[
	\lim_{x \to \bar{x}} \bigg( \lim_{n \to \infty} f_n(x)\bigg) = \lim_{n \to \infty} \bigg( \lim_{x \to \bar{x}} f_n(x)\bigg).
\]
As we have seen, for pointwise convergent this interchange may fail because $f$ need not be continuous. However, Theorem \ref{theo*cont_unif_conv} ensures that this equality holds under uniform convergence.




